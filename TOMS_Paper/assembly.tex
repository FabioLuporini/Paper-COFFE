% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{tabulary}
\usepackage{fancybox}

% font in listings
\usepackage{courier}

\makeatletter
\newenvironment{CenteredBox}{% 
\begin{Sbox}}{% Save the content in a box
\end{Sbox}\centerline{\parbox{\wd\@Sbox}{\TheSbox}}}% And output it centered
\makeatother

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{lineno}
\usepackage{xfrac}

% Theorems, definitions, lemmas, etc
\newtheorem{Def}{Definition}
\newtheorem{Prop}{Proposition}
\newtheorem{Algo}{Algorithm}
\newtheorem{Const}{Constraint}
\newtheorem*{Rem*}{Remark}

\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}

% line over text in non math mode
\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\usepackage{listings}
\lstset{language=C, breaklines=true, mathescape,
			 morekeywords={each,min,not}}

\usepackage[cmex10]{amsmath}
\usepackage{url}

% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}

% Document starts
\begin{document}

% Page heads
\markboth{F. Luporini et al.}{On Optimality of Finite Element Integration}

% Title portion
\title{On Optimality of Finite Element Integration}
\author{Fabio Luporini
\affil{Imperial College London}
David A. Ham
\affil{Imperial College London}
Paul H. J. Kelly
\affil{Imperial College London}}

\begin{abstract}
We tackle the problem of automatically generating optimal finite element integration routines given a high level specification of arbitrary multilinear forms. Optimality and quasi-optimality are defined in terms of floating point operations given a memory bound. We provide an approach to explore the space of legal transformations and discuss under what conditions optimality or quasi-optimality hold. A theoretical analysis and extensive experimentation, which shows systematic performance improvements over several state-of-the-art code generation systems, validate the approach.
\end{abstract}

\category{G.1.8}{Numerical Analysis}{Partial Differential Equations -
  Finite element methods}

\category{G.4}{Mathematical Software}{Parallel and vector implementations}

\terms{Design, Performance}

\keywords{Finite element integration, local assembly, compilers, performance optimization}

\acmformat{Fabio Luporini, David A. Ham, and Paul H. J. Kelly, 2015. On Optimality of Finite Element Integration.}

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT: Full first names whenever they are known, surname last,
% followed by a period.  In the case of two authors, 'and' is placed
% between them.  In the case of three or more authors, the serial
% comma is used, that is, all author names except the last one but
% including the penultimate author's name are followed by a comma, and
% then 'and' is placed before the final author's name.  If only first
% and middle initials are known, then each initial is followed by a
% period and they are separated by a space.  The remaining information
% (journal title, volume, article number, date, etc.) is
% 'auto-generated'.

\begin{bottomstuff}

This research is partly funded by the MAPDES project, by the
Department of Computing at Imperial College London, by EPSRC through
grants EP/I00677X/1, EP/I006761/1, and EP/L000407/1, by NERC grants
NE/K008951/1 and NE/K006789/1, by the U.S.  National Science
Foundation through grants 0811457 and 0926687, by the U.S. Army
through contract W911NF-10-1-000, and by a HiPEAC collaboration
grant. The authors would like to thank Mr. Andrew T.T. McRae,
Dr. Lawrence Mitchell, and Dr. Francis Russell for their invaluable
suggestions and their contribution to the Firedrake project.

Author's addresses: Fabio Luporini $\&$ Paul H. J. Kelly, Department of Computing,
Imperial College London; David A. Ham, Department of Computing and
Department of Mathematics, Imperial College London; 
\end{bottomstuff}

\maketitle


\section{Introduction}

The need for rapid implementation of high performance, robust, and portable finite element methods has led to approaches based on automated code generation. This has been proven successful in the context of the FEniCS (\cite{Fenics}) and Firedrake (\cite{firedrake-code}) projects. In these frameworks, the weak variational form of a problem is expressed at high level by means of a domain-specific language. The mathematical specification is manipulated by a form compiler that generates a representation of assembly operators. By applying these operators to an element in the discretized domain, a local matrix and a local vector, which represent the contributions of that element to the equation approximated solution, are computed. The code for assembly operators must be carefully optimized: as the complexity of a variational form increases, in terms of number of derivatives, pre-multiplying functions, or polynomial order of the chosen function spaces, the operation count increases, with the result that assembly often accounts for a significant fraction of the overall runtime. 

As demonstrated by the substantial body of research on the topic, automating the generation of such high performance implementations poses several challenges. This is a result of the complexity inherent in the mathematical expressions involved in the numerical integration, which varies from problem to problem, and the particular structure of the loop nests enclosing the integrals. General-purpose compilers, such as \emph{GNU's} and \emph{Intel's}, fail at exploiting the structure inherent in the expressions, thus producing sub-optimal code (i.e., code which performs more floating-point operations, or ``flops'', than necessary; we show this is Section~\ref{sec:perf-results}). Research compilers, for instance those based on polyhedral analysis of loop nests such as PLUTO (\cite{PLUTO}), focus on parallelization and optimization for cache locality, treating issues orthogonal to the question of minimising flops. The lack of suitable third-party tools has led to the development of a number of domain-specific code transformation (or synthesizer) systems. In~\citeN{quadrature1}, it is shown how automated code generation can be leveraged to introduce optimizations that a user should not be expected to write ``by hand''. In~\citeN{FFC-TC} and~\citeN{Francis}, mathematical reformulations of finite element integration are studied with the aim of minimizing the operation count. In~\citeN{Luporini}, the effects and the interplay of generalized code motion and a set of low level optimizations are analysed. It is also worth mentioning an on-going effort to produce a new form compiler, called UFLACS (\cite{Uflacs}), which adds to the already abundant set of code transformation systems for assembly operators. The performance evaluation in Section~\ref{sec:perf-results} includes most of these optimization systems.

However, in spite of such a considerable research effort, still there is no answer to one fundamental question: can we automatically generate an implementation of a form which is optimal in the number of flops executed? In this paper, we formulate an approach that solves this problem for a particular class of forms and provides very good approximations (``quasi-optimality'') in all other cases. Summarizing, our contributions are as follows:

\begin{itemize}
\item We formalize finite element integration loop nests and we build the space of legal transformations impacting their operation count.
\item We provide an algorithm to select points in the transformation space. The algorithm uses a cost model to: (i) understand whether a transformation reduces or increases the operation count; (ii) choose between different (non-composable) transformations.
\item We explain under what conditions our algorithm leads to optimality. In particular, we show that (i) for a particular class of problems, optimality is reached; (ii) quasi-optimality is, in general, always achieved (i.e., the operation count is at least optimal in innermost loops).
\item We integrate our approach with a compiler, COFFEE\footnote{COFFEE stands for COmpiler For Fast Expression Evaluation. The compiler is open-source and available at \url{https://github.com/coneoproject/COFFEE}}, which is in use in the Firedrake framework.
\item We experimentally evaluate using a broader suite of forms, discretizations, and code generation systems than has been used in prior research. This is essential to demonstrate that our optimality model holds in practice.
\end{itemize}

In addition, in order to place COFFEE on the same level as other code generation systems from the viewpoint of low level optimization (which is essential for a fair performance comparison)

\begin{itemize}
\item We introduce an engine based on symbolic execution that allows skipping irrelevant floating point operations (e.g., those involving zero-valued quantities).
\end{itemize}


\section{Preliminaries}
\label{sec:background}
We review finite element integration using the same notation and examples adopted in~\citeN{quadrature1} and~\citeN{Francis}. 

We consider the weak formulation of a linear variational problem
\begin{equation}
\begin{split}
\text{Find}\ u \in U\ \text{such that} \\
a(u, v) = L(v), \forall v \in V
\end{split}
\end{equation}
where $a$ and $L$ are, respectively, a bilinear and a linear form. The set of \textit{trial} functions $U$ and the set of \textit{test} functions $V$ are discrete function spaces. For simplicity, we assume $U = V$. Let $\lbrace \phi_i \rbrace$ be the set of basis functions spanning $U$. The unknown solution $u$ can be approximated as a linear combination of the basis functions $\lbrace \phi_i \rbrace$. From the solution of the following linear system it is possible to determine a set of coefficients to express $u$:
\begin{equation}
Au = b
\end{equation}
in which $A$ and $b$ discretize $a$ and $L$ respectively:
\begin{equation}
\centering
\begin{split}
A_{ij} = a(\phi_i(x), \phi_j(x)) \\
b_i = L(\phi_i(x))
\end{split}
\end{equation}
The matrix $A$ and the vector $b$ are ``assembled'' and subsequently used to solve the linear system through (typically) an iterative method.

We focus on the assembly phase, which is often characterized as a two-step procedure: \textit{local} and \textit{global} assembly. Local assembly is the subject of this article. It consists of computing the contributions of a single element in the discretized domain to the equation approximated solution. In global assembly, such local contributions are ``coupled'' by suitably inserting them into $A$ and $b$. 

We illustrate local assembly in a concrete example, the evaluation of the local element matrix for a Laplacian operator. Consider the weighted Laplace equation
\begin{equation}
- \nabla \cdot (w \nabla u) = 0
\end{equation}
in which $u$ is unknown, while $w$ is prescribed. The bilinear form associated with the weak variational form of the equation is:
\begin{equation}
a(v, u) = \int_\Omega w \nabla v \cdot \nabla u\ \mathrm{d}x
\end{equation}
The domain $\Omega$ of the equation is partitioned into a set of cells (elements) $T$ such that $\bigcup T = \Omega$ and $\bigcap T = \emptyset$. By defining $\lbrace \phi_i^K \rbrace$ as the set of local basis functions spanning $U$ on the element $K$, we can express the local element matrix as
\begin{equation}
\label{stiffness}
A_{ij}^K = \int_K w \nabla \phi_i^K \cdot \nabla \phi_j^K\ \mathrm{d}x
\end{equation}
The local element vector $L$ can be determined in an analogous way. 

\subsection{Monomials}
\label{sec:monomials}
In general, it has been shown (e.g., in~\citeN{Kirby:TC}) that local element tensors can be expressed as a sum of integrals over $K$, each integral being the product of derivatives of functions from sets of discrete spaces and, possibly, functions of some spatially varying coefficients. An integral of this form is called \textit{monomial}. 

%From the computational perspective, its evaluation is however less expensive than that of $A$.

\subsection{Quadrature mode}
Quadrature schemes are typically used to numerically evaluate $A_{ij}^K$. For convenience, a reference element $K_0$ and an affine mapping $F_K : K_0 \rightarrow K$ to any element $K \in T$ are introduced. This implies that a change of variables from reference coordinates $X_0$ to real coordinates $x = F_K (X_0)$ is necessary any time a new element is evaluated. The numerical integration routine based on quadrature over an element $K$ can be expressed as follows
\begin{equation}
\label{eq:quadrature}
A_{ij}^K = \sum_{q=1}^N \sum_{\alpha_3=1}^n \phi_{\alpha_3}(X^q)w_{\alpha_3} \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\beta=1}^d \frac{\partial X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial \phi_i^K(X^q)}{\partial X_{\alpha_1}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \frac{\partial \phi_j^K(X^q)}{\partial X_{\alpha_2}} det F_K' W^q
\end{equation}
where $N$ is the number of integration points, $W^q$ the quadrature weight at the integration point $X^q$, $d$ is the dimension of $\Omega$, $n$ the number of degrees of freedom associated to the local basis functions, and $det$ the determinant of the Jacobian matrix used for the aforementioned change of coordinates.  

% Magari la summation sui coefficients la sputo dentro? che dici?

\subsection{Tensor contraction mode}
\label{sec:tc}
Starting from Equation~\ref{eq:quadrature}, exploiting linearity, associativity and distributivity of the involved mathematical operators, we can rewrite the expression as
\begin{equation}
\label{eq:tensor}
A_{ij}^K = \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\alpha_3=1}^n det F_K' w_{\alpha_3} \sum_{\beta=1}^d \frac{X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \int_{K_0} \phi_{\alpha_3} \frac{\partial \phi_{i_1}}{\partial X_{\alpha_1}} \frac{\partial \phi_{i_2}}{\partial X_{\alpha_2}} dX.
\end{equation}
A generalization of this transformation has been proposed in~\cite{Kirby:TC}. Because of only involving reference element terms, the integral in the equation can be pre-evaluated and stored in temporary variables. The evaluation of the local tensor can then be abstracted as
\begin{equation}
A_{ij}^K = \sum_{\alpha} A_{i_1 i_2 \alpha}^0 G_{K}^\alpha
\end{equation}
in which the pre-evaluated ``reference tensor'' $A_{i_1 i_2 \alpha}$ and the cell-dependent ``geometry tensor'' $G_{K}^\alpha$ are exposed. 

\subsection{Qualitative comparison}
\label{sec:qualitative}
Depending on form and discretization, the relative performance of the two modes, in terms of the operation count, can vary quite dramatically. The presence of derivatives or coefficient functions in the input form tends to increase the size of the geometry tensor, making the traditional quadrature mode preferable for ``complex'' forms. On the other hand, speed-ups from adopting tensor mode can be significant in a wide class of forms in which the geometry tensor remains ``sufficiently small''. The discretization, particularly the relative polynomial order of trial, test, and coefficient functions, also plays a key role in the resulting operation count. 

These two modes have been implemented in the FEniCS Form Compiler (\cite{FFC-TC}). In this compiler, a heuristic is used to choose the most suitable mode for a given form. It consists of analysing each monomial in the form, counting the number of derivatives and coefficient functions, and checking if this number is greater than a constant found empirically (\cite{Fenics}). We will later comment on the efficacy of this approach (Section~\ref{sec:perf-results}). For the moment, we just recall that one of the goals of this research is to produce a system that goes beyond the dichotomy between quadrature and tensor modes. We will reason in terms of loop nests, code motion, and code pre-evaluation, searching the entire implementation space for an optimal synthesis.  

\section{Transformation Space}
\label{sec:optimal-impl}
In this section, we characterize optimality and quasi-optimality for finite element integration as well as the space of legal transformations that we need to explore to achieve it. How the exploration is performed is discussed in Section~\ref{sec:optimal-synthesis}. 

\subsection{Loop nests, expressions and optimality}
\label{sec:lnopt}
In order to make the article self-contained, we start with reviewing basic compiler terminology.

\begin{Def}[Perfect and imperfect loop nests]
A perfect loop nest is a loop whose body either 1) comprises only a sequence
of non-loop statements or 2) is itself a perfect loop nest. If this
condition does not hold, a loop nest is said to be imperfect. 
\end{Def}

\begin{Def}[Independent basic block]
An independent basic block is a sequence of statements such that no data
dependencies exist between statements in the block.
\end{Def}

We focus on perfect nests whose innermost loop body is an independent basic
block. A straightforward property of this class is that hoisting invariant
expressions from the innermost to any of the outer loops or the preheader
(i.e., the block that precedes the entry point of the nest) is always safe,
as long as any dependencies on loop indices are honored. We will make use of this property. The results of this section could also be generalized to larger classes of loop nests, in which basic block independence does not hold, although this would require refinements beyond the scope of this paper. 

By mapping mathematical properties to the loop nest level, we introduce the
concepts of a \textit{linear loop} and, more generally, a (perfect) multilinear loop nest.

\begin{Def}[Linear loop]
A loop $L$ defining the iteration space $I$ through the iteration variable $i$, or simply $L_i$, is linear if in its body
\begin{enumerate}
\item $i$ appears only as an array index, and
\item whenever an array $a$ is indexed by $i$ ($a[i]$), all expressions in which this appears are affine in $a$.
\end{enumerate}
\end{Def}

\begin{Def}[Multilinear loop nest]
A multilinear loop nest of arity $n$ is a perfect nest composed of $n$ loops, in which all of the expressions appearing in the body of the innermost loop are linear in each loop $L_i$ separately.
\end{Def}

We will show that multilinear loop nests, which arise naturally when translating bilinear or linear forms into code, are important because they have a structure that we can take advantage of to synthesize optimal code.

We define two other classes of loops. 

\begin{Def}[Reduction loop]
\label{def:i-loop}
A loop $L_i$ is said to be a reduction loop if in its body
\begin{enumerate}
\item $i$ appears only as an array index, and
\item for each augmented assignment statement $S$ (e.g., an increment), arrays indexed by $i$ appear only on the right hand side of $S$.
\end{enumerate}
\end{Def}

\begin{Def}[Free order loop]
\label{def:e-loop}
A loop $L_i$ is said to be a free order loop if its iterations can be executed in any arbitrary order; that is, there are no loop-carried dependencies across different iterations. 
\end{Def}

\begin{figure}\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest.code}
\end{CenteredBox}\caption{The loop nest implementing a generic bilinear form.}\label{code:loopnest}\end{figure}

To contextualize, consider Equation~\ref{eq:quadrature} and the (abstract) loop nest implementing it illustrated in Figure~\ref{code:loopnest}. The imperfect nest $\Lambda=[L_e, L_i, L_j, L_k]$ comprises a free order loop $L_e$ (over elements in the mesh), a reduction loop $L_i$ (performing numerical integration), and a multilinear loop nest $[L_j, L_k]$ (over test and trial functions). In the body of $L_k$, one or more statements evaluate the local tensor for the element $e$. Expressions (right hand side of a statement) result from the translation of a form in high level matrix notation into code. In particular, $m$ is the number of ``terms'' (a monomial can be implemented by one or more terms), $\alpha_{eij}$ ($\beta_{eik}$) represents the product of a coefficient function (e.g., the inverse Jacobian matrix for the change of coordinates) with test (trial) functions, and $\sigma_{ei}$ is a function of coefficients and geometry. We do not pose particular restrictions on function spaces (e.g., scalar- or vector-valued), coefficients (e.g., linear or non-linear), differential and vector operators, so $\sigma_{ei}$ can be arbitrarily complex. We say that such an expression is in \textit{normal form}, because the algebraic structure of a variational form is intact (e.g., products have not been expanded yet, distinct monomials can still be identified, etc.). This bring us to formalize the class of loop nests for which we seek optimality.

%TODO is "not expanded yet == still contracted" ?

\begin{Def}[Finite element integration loop nest]
\label{def:fem-loopnest}
A finite element integration loop nest is a loop nest in which we identify, in order, an imperfect free order loop, a (generally) imperfect, linear or non-linear reduction loop, and a multilinear loop nest whose body is an independent basic block in which each statement has expressions in normal form.
\end{Def}

For a finite element integration loop nest, we characterize optimality and quasi-optimality as follows.

\begin{Def}[Optimality of a loop nest]
\label{def:mln-optimality}
Let $\Lambda$ be a generic loop nest, and let $\Gamma$ be a generic transformation function $\Gamma : \Lambda \rightarrow \Lambda'$ such that $\Lambda'$ is semantically equivalent to $\Lambda$ (possibly, $\Lambda' = \Lambda$). We say that $\Lambda' = \Gamma (\Lambda)$ is an optimal synthesis of $\Lambda$ if the number of operations (additions, products) that it performs to evaluate the result is minimal.
\end{Def}

\begin{Def}[Quasi-optimality of a loop nest]
\label{def:mln-quasi-optimality}
Given $\Lambda$, $\Lambda'$ and $\Gamma$ as in Definition~\ref{def:mln-optimality}, we say that $\Lambda' = \Gamma (\Lambda)$ is a quasi-optimal synthesis of $\Lambda$ if the number of operations (additions, products) that it performs to evaluate the result is at least minimal in all innermost loops.
\end{Def}

Note that Definitions~\ref{def:mln-optimality} and~\ref{def:mln-quasi-optimality} do not take into account memory requirements. If the loop nest were memory-bound -- the ratio of operations to bytes transferred from memory to the CPU being too low -- then speaking of optimality would clearly make no sense. Henceforth we assume to operate in a CPU-bound regime, in which arithmetic-intensive expressions need be evaluated. In the context of finite element, this is often true for more complex multilinear forms and/or higher order elements. We also note that quasi-optimality approximates optimality very well whenever the inner loop trip counts are ``sufficiently large''.

Achieving optimality in polynomial time is not generally feasible, since the $\sigma_{ei}$ sub-expressions can be arbitrarily unstructured. On the other hand, multilinearity ensures a certain degree of regularity for the  $\alpha_{eij}$ and $\beta_{eik}$ sub-expressions, which is reflected in the innermost loop. In the following sections, we will elaborate on these observations and formulate an approach that achieves: (i) optimality whenever the $\sigma_{ei}$ sub-expressions are ``sufficiently regular'';  (ii) quasi-optimality in all other cases. To this purpose, we will construct:
\begin{itemize}
\item the space of legal transformations impacting the operation count (Sections~\ref{sec:sharing-elimination} -- \ref{sec:completeness})
\item an algorithm to select points in the transformation space (Section~\ref{sec:optimal-synthesis})
\end{itemize}

\subsection{Sharing elimination}
\label{sec:sharing-elimination}
We start with introducing the fundamental notion of sharing.

\begin{Def}[Sharing]
A statement within a loop nest $\Lambda$ presents sharing if at least one of the following conditions hold:
\begin{enumerate}
\item there are at least two symbolically identical sub-expressions (spatial sharing)
\item there is at least one non-trivial sub-expression (an addition or a product) that is redundantly executed as independent of $\lbrace L_{i_0}, L_{i_1}, ...L_{i_n} \rbrace \subset \Lambda$ (temporal sharing)
\end{enumerate}
\end{Def}

To illustrate the definition, we show in Figure~\ref{code:multi_loopnest} how sharing evolves as factorization and code motion are applied to a trivial multilinear loop nest. In the original loop nest (Figure~\ref{code:multi_loopnest_a}), spatial sharing is induced by $b_j$. Factorization eliminates spatial sharing and promotes temporal sharing (Figure~\ref{code:multi_loopnest_b}). Finally, generalized code motion (\cite{Luporini}) leads to optimality (Figure~\ref{code:multi_loopnest_c}). 

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 7pt \subfigure[With spatial sharing]{\label{code:multi_loopnest_a}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest.code}}}
~~~~~
{\subfigcapskip = 7pt \subfigure[With temporal sharing]{\label{code:multi_loopnest_b}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest_int.code}}}
~~~~~
{\subfigcapskip = 2pt \subfigure[Optimal form]{\label{code:multi_loopnest_c}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest_opt.code}}}
\end{CenteredBox}\caption{Reducing a simple multilinear loop nest to optimal form.}\label{code:multi_loopnest}\end{figure}

In this section, we study \textit{sharing elimination}, a transformation that aims to reduce the operation count by removing sharing through the application of factorization, generalized code motion and (if necessary) common sub-expression elimination. If the objective were reaching optimality and expressions were systematically unstructured, a transformation of this sort would require solving a large combinatorial problem -- for instance to evaluate the impact of all possible factorizations. Our sharing elimination strategy, instead, attempts to exploit the structure inherent in finite element integration expressions to guarantee quasi-optimality\footnote{This require coordination with other transformations, as discussed in the following sections. For the moment -- and for ease of treatise -- we neglect this aspect.}, with optimality being achieved if stronger hypotheses hold. Relaxing the problem is essential to produce simple and computationally efficient algorithms -- two necessary conditions for integration with a compiler. 

Sections~\ref{sec:se-mln} and~\ref{sec:se-rln} discuss the structural (i.e., inherent in finite element integration) and algebraic (i.e., related to the use of differential and vector operators) properties that can characterize our expressions. Section~\ref{sec:sec:se-algo} presents the sharing elimination algorithm. We recall that we are assuming a generic finite element integration loop nest $\Lambda=[L_e, L_i, L_j, L_k]$ with expressions in normal form $\sum_{w=1}^m \alpha_{eij}^{w} \beta_{eik}^{w} \sigma_{ei}^{w}$.

\subsubsection{Common sub-expressions}
\label{sec:se-mln}
A typical optimization to reduce the arithmetic complexity of a code is common sub-expression elimination (CSE). This often revolves around searching, possibly across different statements, and replacing with temporaries, sub-expressions that are (i) symbolically identical and (ii) reduce to the same value.  We observe that ``classic CSE'' (CCSE) can straightforwardly be applied without the need for data dependence analysis, since the body of a finite element integration loop nest is an independent basic block. More interestingly, we observe that in the ``special case'' (SCSE) in which test and trial functions are in the same function space, there may exist $w_1$ and $w_2$ such that $\alpha_{eij}^{w1} = \beta_{eik}^{w2}$, in which case common sub-expressions will arise along different loops ($L_j$ and $L_k$). General-purpose compilers cannot detect this (very domain-specific) property. Our algorithm for sharing elimination will exploit this extra information. Although both CSE and factorization target spatial sharing, the latter may actually increase the operation count if preliminary transformations (e.g., expansion) are required. In such a case, CCSE will be preferred.

\subsubsection{Structured tensor operations}
\label{sec:se-rln}
Finite element expressions can be seen as composition of operations between tensors. We observe that, when multiplying two tensors, it often happens that the optimal scheduling strategy is to be searched in a space of size 2. To illustrate the concept, we take a matrix-vector product $y_{\Psi \Delta} = A_{\Psi} x_{\Delta}$, in which the entries of $y, A$ and $x$ depend on $\Psi, \Delta \subset \Lambda$. Let us assume that $y_{\Psi \Delta}$ itself is an operand of a larger scalar-valued expression, so its entries can represent any possible sub-expression in $\lbrace \alpha_{eij}, \beta_{eik}, \sigma_{ei} \rbrace$. For simplicity, assume that $A$ is the inverse Jacobian matrix of a given coefficient and $x$ the gradient of a function in two dimensions, and consider the case $\Psi = \emptyset$, $\Delta = \lbrace L_i \rbrace$. Thus an entry in $y_{L_i}$ is of the form $(a f_i + b g_i) (c f_i + d g_i) \in \sigma_{ei}$. To schedule this expression, we basically have two options:
\begin{enumerate}
\item We leave the expression unmodified in the body of $L_i$ (in general, one may consider applying generalized code motion or creating a temporary for CCSE -- but it is not the case of this simple example).
\item We notice the presence of spatial sharing. We can recast the expression as $f_i (a + c) + g_i (b + d)$, which promotes temporal sharing and, therefore, code motion. 
\end{enumerate}
The reader can verify that option 2) improves the operation count over 1) if and only if the size of $L_i$ is strictly greater than $c=1$. One can appreciate this dichotomy in a wide range of variational forms\footnote{In fact, it systematically appears in all of the forms used for experimentation (Section~\ref{sec:perf-results}), and many more.}, obviously with varying values of $c$. 

In general, given a product of two sub-expressions in a finite element integration loop nest, we observe that whenever there is no ambiguity in the elimination of spatial sharing, the problem of finding an optimal scheduling reduces to the comparison of two alternatives. We note that this condition simply holds if a loop is guaranteed to be linear; in the next section, we will show how to take advantage of this property to ensure quasi-optimal sharing elimination. Further, if the $\sigma_{ei}$ expressions are sufficiently regular, the same approach can be applied to all of the $L_i$-dependent expressions. Structured tensor operations within $\sigma_{ei}$ are not as uncommon as one may think in the first place: for instance, they systematically arise in the weak variational form of the complex hyperelastic model analyzed in Section~\ref{sec:perf-results} (e.g., the Cauchy-Green tensor). 

%It is beyond the scope of this paper, but one could think of formalizing the conditions for which sigma_ei is structured...


\subsubsection{Algorithm}
\label{sec:sec:se-algo}
Algorithm~\ref{algo:opt} details the steps for performing sharing elimination on a tree representation of the loop nest. The algorithm is meant to be applied to each statement appearing in the independent basic block of the input finite element integration loop nest. It exploits the observation that test and trial functions are always part of structured tensor operations, due to the nature of finite element integration expressions.

\begin{Algo}[Sharing elimination]
\label{algo:opt}
\normalfont
The initialization step consists of a depth-first visit of the expression tree with recursive collection of identical sub-expressions, to ensure the uniqueness of operands. For example, the expression $(a + a + b + c + a)$ is transformed into $(3a + b + c)$. 

We perform a second depth-first visit of the expression tree to collect information about multilinear ``symbols'' and ``operands''. For example, any sub-expression $\alpha_{eij}$ ($\beta_{eik}$) is regarded as an operand, while a symbol could be any test (trial) function (or derivatives of) $a_{ij} \in \alpha_{eij}$. In particular, for each symbol $s$, we store a 2-tuple ${<}n,\ \lbrace ops \rbrace{>}$, where $n$ is the number of occurrences of $s$ in the input expression, while $\lbrace ops \rbrace$ represents the set of operands in which $s$ appears. 

Let $\mathbb{O}$ be the set of all $\lbrace ops \rbrace$s. 


%ops\ =\ \lbrace \alpha_{eij}^{1}, ..., \alpha_{eij}^{N}\rbrace$

%and we recursively label each node $n$ with a 2-tuple ${<}I_{jk},\ op_{gcm}{>}$, where: $I_{jk}$ represents the intersection of the operands depending on the multilinear loop nest within $n$, while $op_{gcm}$ denotes the gain in operation count if generalized code motion were applied to all sub-expressions rooted in $n$ (i.e., if they were placed sufficiently far in the loop nest such that redundant computation were avoided). A crucial observation is that $I_{jk}$ is, in practice, always very small (a few units), even if the form is complex and includes several monomials. In addition, its computation can be made extremely efficient by also tracking the union of the symbols rooted in $n$.
%
%On a subsequent depth-first visit, we examine the label of each operator \texttt{+}. If $I_{jk} = \emptyset$ -- that is, no spatial sharing -- we just proceed to the parent node. Otherwise, we speculatively assume that structured tensor operations are being performed in the sub-tree rooted in \texttt{+}, and compare the operational cost of the two alternatives described in Section~\ref{sec:se-rln} (see points (1) and (2)). Alternative (1) has an operation count equal to $op_{gcm}$. Alternative (2) requires collecting the shared symbols and estimating the impact of generalized code motion on the new factorization (similarly to Figure~\ref{code:multi_loopnest}). For this, we need decide the collection order; that is, whether to start with symbols in $I_j$ or $I_k$. It can easily be demonstrated by contradiction that starting with $I_o$ such that $|L_o| |I_o|$ is minimum ($o \in {\lbrace} j, k{\rbrace}$, with $|.|$ representing a generic ``\texttt{sizeof}'' operator) leads to the lowest operation count. Once all of the operands rooted in Alternative (2) is immediately applied if determined to be ; alternative (1) is instead applied only after all operands rooted in \texttt{+} have been examined. 

%TODO sharing elimination of Li dependent stuff

%TODO CSE / CCSE /  SCSE In all this process, whenever a sub-expression is hoisted, we search and replace all common sub-expressions. This is probably useless for the actual operation count, since we expect any general-purpose compiler to be able to perform aggressive common sub-expression elimination\footnote{By inspection of assembly code, we have verified that this is actually the case with the Intel compiler v15.2.}, but it helps making the generated code much more readable.
\end{Algo}

\subsection{Pre-evaluation}
\label{sec:pre-evaluation}
Sharing elimination explores the transformation space and applies three operators: factorization, code motion and common sub-expression elimination. In this section, we discuss role and legality of a fourth operator: \textit{reduction pre-evaluation}. We will see that what makes this operator special is the fact that there exists a single point in a monomial transformation space (i.e., specific factorization and code motion) preserving the correctness of the transformation.

We start with an example. Consider again the bilinear form implementation in Figure~\ref{code:loopnest}. We pose the following question: are we able to identify sub-expressions within $F$ for which the reduction imposed by $L_i$ can be pre-evaluated, thus obtaining a decrease in operation count proportional to the size of $L_i$, $M$? The transformation we look for is exemplified in Figures~\ref{code:loopnest_red} and~\ref{code:loopnest_nored}; Figure~\ref{code:loopnest_red}, which represents the input, can be seen as a simple instance of the abstract loop nest in Figure~\ref{code:loopnest}.

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 19pt \subfigure[With reduction]{\label{code:loopnest_red}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_red.code}}}
~~~~~~~~~~
{\subfigcapskip = 2pt \subfigure[Pre-evaluated reduction]{\label{code:loopnest_nored}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_nored.code}}}
\end{CenteredBox}\caption{Exposing (through factorization) and pre-evaluating a reduction.}\label{code:loopnest_rednored}\end{figure}

Pre-evaluation opportunities can be exposed through exploration of the expression tree transformation space. This would be challenging if we were to deal with arbitrary loop nests and expressions. However, we make use of a result -- the foundation of tensor contraction mode -- to simplify our task. As summarized in Section~\ref{sec:tc}, multilinear forms can be seen as sums of monomials, each monomial being an integral over the equation domain of products (of derivatives) of functions from discrete spaces; such monomials can always be reduced to a product of two tensors. This result can be turned into a transformation algorithm for loops and expressions, for which we provide a succinct description below. 

\begin{Algo}[Pre-evaluation]
\label{algo:pre-evaluation}
\normalfont
Consider a finite element integration loop nest $\Lambda = [L_e, L_i, L_j, L_k]$. We dissect \texttt{F} into distinct sub-expressions (the monomials). Each sub-expression is factorized so as to split constants from $[L_i, L_j, L_k]$-dependent terms. This transformation is feasible, as a consequence of the results in~\citeN{Kirby:TC}. These $[L_i, L_j, L_k]$-dependent terms are hoisted outside of $\Lambda$ and stored into temporaries. As part of this process, the reduction induced by $L_i$ is evaluated. Consequently, $L_i$ disappears from $\Lambda$. 
\end{Algo}

The pre-evaluation of a monomial introduces some critical issues:
\begin{enumerate}
\item In contrast to what happens with hoisting in multilinear loop nests, the temporary variable size is proportional to the number and trip counts of non-reduction loops crossed (for the bilinear form implementation in Figure~\ref{code:loopnest}, $N O$ for sub-expressions depending on $[L_i, L_j, L_k]$ and $L N O$ for those depending on $[L_e, L_i, L_j, L_k] $). This might shift the loop nest from a CPU-bound to a memory-bound regime, which might be counter-productive for actual execution time.
\item The transformations exposing $[L_i, L_j, L_k]$-dependent terms increase, in general, the arithmetic complexity (e.g., expansion may increase the operation count). This could outweigh the gain due to pre-evaluation.
\item The need for a strategy to coordinate sharing elimination and pre-evaluation opportunities: sharing elimination inhibits pre-evaluation, whereas pre-evaluation generally exposes further sharing elimination opportunities.
\end{enumerate}

We expand on point 1) in the next section. We address points 2) and 3) in Section~\ref{sec:optimal-synthesis}. 

\subsection{Memory constraints}
\label{sec:mem-const}

In the previous section, we provided an insight into the potentially negative effects of code motion. In this section, we expand on this matter. The two following observations, in particular, lead to the definition of \textit{memory constraints}.

\begin{itemize}
\item The fact that $L \gg M, N, O$ suggests we should be cautious about hoisting mesh-dependent (i.e., $L_e$-dependent) expressions. Imagine $\Lambda$ is enclosed in a time stepping loop. One could think of exposing (through some transformations) and hoisting any time-invariant sub-expressions to minimize redundant computation at every time step. The working set size could then increase by a factor $L$. The gain in number of operations executed could therefore be outweighed, from a runtime viewpoint, by a much larger memory pressure.
\item For certain forms and discretizations, aggressive hoisting can make the working set exceed the size of some level of local memory (e.g. the last level of private cache on a conventional CPU, the shared memory on a GPU). For example, as explained in Section~\ref{sec:pre-evaluation}, pre-evaluating geometry-independent expressions outside of $\Lambda$ requires temporary arrays of size $N O$ for bilinear forms and of size $N$ (or $O$) for linear forms. This can sometimes break such a ``local memory threshold''. In our experiments (Section~\ref{sec:perf-results-forms}) we will carefully study this aspect.
\end{itemize}

Based on these considerations, we establish the following memory constraints.

\begin{Const}
\label{const:Le}
The size of a temporary due to code motion must not be proportional to the size of $L_e$.
\end{Const}

\begin{Const}
\label{const:TH}
The total amount of memory occupied by the temporaries due to code motion must not exceed a certain threshold, \texttt{$T_H$}.
\end{Const}

Constraint~\ref{const:Le} reflects the policy decision that the compiler should not silently consume memory on ... data objects. Consequently, generalized code motion as performed by sharing elimination is not allowed to hoist expressions outside of $L_e$. 

We conclude providing a more refined definition of optimality.

\begin{Def}[Optimality of a loop nest with bounded working set]
\label{def:optimality}
Let $\Lambda$ be a generic loop nest, and let $\Gamma$ be a generic transformation function $\Gamma : \Lambda \rightarrow \Lambda'$ such that $\Lambda'$ is semantically equivalent to $\Lambda$ (possibly, $\Lambda' = \Lambda$) and Constraints~\ref{const:Le} and~\ref{const:TH} are satisfied. We say that $\Lambda' = \Gamma (\Lambda)$ is an optimal synthesis of $\Lambda$ if the number of operations (additions, products) that it performs to evaluate the result is minimal.
\end{Def}

An analogous definition can be provided for quasi-optimality.

\subsection{Completeness under factorization, code motion, and reduction pre-evaluation}
\label{sec:completeness}
We defined sharing elimination and pre-evaluation as high level transformations on top of basic operators such as code motion and factorization. Factorization addresses \textit{spatial redundancy}. The presence of spatial redundancy means that some operations are needlessly executed at two points in an expression. Code motion and reduction pre-evaluation, on the other hand, target \textit{temporal redundancy}; that is, the needless execution of the same operation with the same operands at two points in the loop nest.

Sharing elimination and pre-evaluation tackle the problem of minimizing spatial and temporal redundancy given a set of memory constraints in finite element integration loop nests, using \textit{a very specific set of operators}. This needs be emphasized since, theoretically, one could find an even lower operation count by exploiting domain-specific properties, such as redundancies in basis functions.

\section{Selection and composition of transformations}
\label{sec:optimal-synthesis}
In this section, we build a transformation algorithm that produces optimal or quasi-optimal finite element integration loop nests. In the perspective of integrating the transformation algorithm with a code generation system, we also discuss an approach that trades optimality for compilation time.

\subsection{Transformation algorithm}

At this point, two main issues are to be addressed: 
\begin{enumerate}
\item \textit{Coordinating the application of pre-evaluation and sharing elimination.} Recall from Section~\ref{sec:pre-evaluation} that pre-evaluation could either increase or decrease the operation count with respect to sharing elimination.
\item \textit{Finding the global optimum.} Consider a form comprising two monomials $m1$ and $m2$. Assume that pre-evaluation is profitable for $m1$ but not for $m2$, and that $m1$ and $m2$ share at least one term (e.g. some basis functions). If pre-evaluation were applied to $m1$, sharing between $m1$ and $m2$ would be lost. We then need a mechanism to understand what transformation -- pre-evaluation or sharing elimination -- results in the highest operation count reduction when considering the whole set of monomials (i.e., the expression as a whole).
\end{enumerate}

Let $\theta : M \rightarrow \mathbb{Z}$ be a cost function that, given a monomial $m \in M$, returns the gain/loss achieved by pre-evaluation over sharing elimination. In particular, we define $\theta(m) = \theta_{se}(m) - \theta_{pre}(m)$, where $\theta_{se}$ and $\theta_{pre}$ represent the operation counts resulting from applying sharing elimination and pre-evaluation, respectively. Thus pre-evaluation is profitable for $m$ if and only if $\theta(m) > 0$. We return to the issue of deriving $\theta_{se}$ and $\theta_{pre}$ in Section~\ref{sec:op_count}. Having defined $\theta$, we can focus on the transformation algorithm, whose pseudo-code is provided in Figure~\ref{code:intuition}.

%numbers=left
\begin{figure}[h]
\begin{lstlisting}[numbers=left, basicstyle=\small\ttfamily, frame=single]
// Initialization
M = $\lbrace$m | m is a monomial in the input expression$\rbrace$
S = $\lbrace$m | $\theta(m)$ < 0$\rbrace$
P = M $\setminus$ S

// Find potential optimum such that memory constraints are honored
B = $\lbrace$b | b is a bipartite graph from complete graph G = (P, E)$\rbrace$
for each b $\in$ B:
  for each (b$_S$, b$_P$) $\in$ b:
    if memory_required(b$_P$) $\geq$ $T_H$:
      continue
    op_count[b] = $\theta_{se}$(S $\cup$ b$_S$) + $\theta_{pre}$(b$_P$)
(b$_S$, b$_P$) = min(op_count)

// Apply the transformations
pre_evaluate(P $\cup$ b$_P$)
eliminate_sharing(S $\cup$ b$_S$ $\cup$ P $\cup$ b$_P$)
\end{lstlisting}
\caption{High level view of the transformation algorithm}
\label{code:intuition}
\end{figure}

The algorithm starts with splitting the monomials into two disjoint sets: the monomials that surely do not take advantage of pre-evaluation ($S$) and those that instead potentially benefit from it ($P$). Some subsets of monomials in $P$: 1) might share terms with $S$, in which case sharing elimination could globally outperform pre-evaluation; 2) might break Constraint~\ref{const:TH}. The algorithm then finds the optimal bipartition $b = (b_S, b_p)$ of $P$ such that $b_P$ represents the subset of monomials for which pre-evaluation is globally optimal and Constraint~\ref{const:TH} is honored. Since the number of monomials in a form tends to be very small (a few units in most complex forms), evaluating all possible bipartitions is not a significant issue as long as the cost of calculating $\theta_{se}$ and $\theta_{pre}$ is negligible. This aspect is elaborated in Section~\ref{sec:op_count}. In addition, observe that because of reuse of basis functions, pre-evaluation may result in identical tables, which will be mapped to the same temporary. Therefore, sharing elimination is transparently applied to all monomials (i.e., to all sub-expressions, even those produced by pre-evaluation). The output of the transformation algorithm is as in Figure~\ref{code:loopnest-opt}, assuming as input the loop nest in Figure~\ref{code:loopnest}. 

\begin{figure}\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_opt.code}
\end{CenteredBox}\caption{The loop nest produced by the algorithm for an input as in Figure~\ref{code:loopnest}.}\label{code:loopnest-opt}\end{figure}

In the following section, we tie up the remaining loose end: the construction of the cost function $\theta$.


\subsection{The cost function $\theta$}
\label{sec:op_count}
We recall that $\theta(m) = \theta_{se}(m) - \theta_{pre}(m)$, with $\theta_{se}$ and $\theta_{pre}$ representing the operation counts after applying sharing elimination and pre-evaluation. Since $\theta$ is expected to be used by a compiler, requirements are simplicity and velocity. In the following, we explain how to derive these two values.

The most trivial way of evaluating $\theta_{se}$ and $\theta_{pre}$ is to apply the actual transformations and count the number of operations. If, on one hand, this is definitely possible for $\theta_{se}$ (performing Algorithm~\ref{algo:opt} tends to have negligible cost), the overhead becomes unacceptable when applying pre-evaluation to all possible bipartitions (Algorithm~\ref{algo:pre-evaluation}, due to the symbolic evaluation of $L_i$). We then seek an analytic way of determining $\theta_{pre}$.

The first step consists of estimating the \textit{increase factor}, $\iota$. This number captures the increase in arithmetic complexity due to the transformations enabling pre-evaluation. To contextualize, consider the example in Figure~\ref{code:increase_factor}. One can think of this as the (simplified) loop nest originating from the integration of a pre-multiplied mass matrix. The sub-expression \texttt{$f_0$*$b_{i0}$+$f_1$*$b_{i1}$+$f_2$*$b_{i2}$} represents the coefficient $f$ over (tabulated) basis functions (array $B$). In order to apply pre-evaluation, the expression needs be transformed to separate $f$ from all $L_i$-dependent quantities. By performing product expansion, we observe an increase in the number of $[L_j, L_k]$-dependent terms of a factor $\iota = 3$.

\begin{figure}\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_inc_factor.code}
\end{CenteredBox}\caption{Simplified loop nest for a pre-multiplied mass matrix.}\label{code:increase_factor}\end{figure}

Despite determining $\iota$ is straightforward in simple scenarios (e.g., when the form has a single coefficient, as we just saw), in general we must account for repeated sub-expressions that, once pre-evaluated, would result in identical tables. This is the case, for example, of a monomial having more than one coefficient in the same function space. One can verify this by taking the example in Figure~\ref{code:increase_factor}, adding a second coefficient $g$ over the same basis functions $b$, and expanding the products. To evaluate $\iota$, we then use combinatorics. We calculate the $k$-combinations with repetitions of $n$ elements, where: (i) $k$ is the number of (derivatives of) coefficients appearing in a product (in the example, there is only $f$, so $k=1$); (ii) $n$ is the number of unique basis functions involved in the expansion (in the example, $b_{i0}$, $b_{i1}$, and $b_{i2}$, so $n=3$). 

If $\iota \geq I$ (the extent of the reduction loop), we already know that pre-evaluation will not be profitable. Intuitively, this means that we are introducing more operations than we are saving from pre-evaluating $L_i$. 

If $\iota < I$, we still need to find the number of terms $\rho$ such that $\theta_{pre} = \rho \cdot \iota$. Consider again the mass matrix in Figure~\ref{code:increase_factor}. This monomial is characterized by the dot product of test and trial functions, so trivially $\rho = 1$. However, in general, $\rho$ depends on both form and discretization. If we instead take a standard Poisson equation using P1 Lagrange elements over triangles we have, for example, $\rho = 3$ (this can be verified by factorizing derivatives of basis functions as explained in Algorithm~\ref{algo:pre-evaluation}). To determine $\rho$ we execute the initial phase of Algorithm~\ref{algo:pre-evaluation}, which obviously excludes reduction pre-evaluation.


\subsection{Trading operation count for analysis cost}
We can relax the constraints on (quasi-)optimality to relieve the analysis phase. In this section, we discuss ways of achieving this.
\label{sec:obsheu}
\begin{itemize}
\item \textit{Transformation algorithm.} The transformation algorithm (Figure~\ref{code:intuition}) evaluates all possible combinations in which pre-evaluation candidates can be scheduled to determine the optimal configuration honoring memory constraints. Despite the fact that we have never observed any tangible increase in analysis cost, one could expect slow-downs in complex forms with many monomials. This matter is yet to be properly investigated. However, a possibility is to avoid or constrain the search; in the worst case scenario, this would be equal to reaching local optimums.
\item \textit{Construction of $\theta$.} Instead of determining exact values for $\theta_{se}$ and $\rho$ (for $\theta_{pre}$) by (partly) executing Algorithms~\ref{algo:opt} and~\ref{algo:pre-evaluation}, approximations can be found by exploiting loop linearity. The number of unique terms in a multilinear loop nest can be used to calculate a lower bound on the innermost loop operation count (where, for higher order methods, the bulk of the computation is performed). For example, in Figure~\ref{code:multi_loopnest_a}, only one term depends on $L_j$ ($b_j$), so at most one multiplication is expected in the innermost loop once sharing is eliminated. An approximation for $\theta_{se}$ is then $\theta_{se} = n_{iters} \cdot n_{unique}$, where $n_{iters}$ is the total number of loop iterations, and $n_{unique}$ is the smallest number of unique terms depending on one the linear loops.
\end{itemize}

%TODO forget about the special case alpha=beta (ie test and trial from same fs)

These heuristics have been implemented in the code generation system described in Section~\ref{sec:codegen}.


\section{Code Generation}
\label{sec:codegen}
Sharing elimination, pre-evaluation, the transformation algorithm, and all other features presented in Sections~\ref{sec:optimal-impl} and~\ref{sec:optimal-synthesis} have been implemented in COFFEE, the optimizer for finite element integration routines used in Firedrake. In this section, we discuss implementation and code generation.

\subsection{Automation through the COFFEE language}
COFFEE implements all optimizations by composing ``building-block'' transformations, which we refer to as ``rewrite operators''. This has several advantages. Firstly, extendibility: novel transformations -- for instance, sum-factorization in spectral methods -- could be expressed using the existing operators, or with small effort building on what is already available. Secondly, generality: any sort of codes characterized by (linear, but also non-linear) loop nests with sharing can be optimized (obviously, without guarantees on optimality). In other words, COFFEE only exploits domain properties, and it is not specifically tied to finite element in any way. Thirdly, robustness: the same operators are exploited, and therefore stressed, by different optimization pipelines.

The rewrite operators, which implementation is based on abstract syntax tree manipulation, compose the COFFEE language. A non-exhaustive list of such operators includes expansion, factorization, re-association, generalized code motion. Sharing elimination and pre-evaluation are implemented by applying (composing) in specific sequences these operators.

\subsection{Independence from form compilers}
\label{sec:zeros}
One of the key pillars of COFFEE is independence from form compilers. Being capable of handling generic abstract syntax trees, COFFEE can be leveraged by any suitably adapted form compiler. For example, in Firedrake a modified version of the FEniCS Form Compiler producing abstract syntax trees instead of strings is used. COFFEE itself provides an interface for building such abstract syntax trees. In particular, COFFEE aims to decouple the form manipulation (i.e., the ``math level'') from code optimization (flops executed, low level optimization such as vectorization). Another viewpoint is: developers of form compilers should not worry about expression optimization in any way, since this is handled at a lower level.

To achieve this goal, COFFEE needs a strategy to minimize the execution of potentially ``useless'' flops due to simplistic implementations of non scalar-valued function spaces. For example, the most trivial quadrature mode in the FEniCS Form Compiler implements vector-valued function spaces by populating tabulated basis functions with blocks of zero-valued columns. For the local element matrix/vector evaluation, these blocks are iterated over in a single, rectangular iteration space. This preserves the semantics of the computation while keeping the implementation complexity extremely low, although it also introduces unnecessary operations (e.g., zero-valued summands due to multiplications by zero). COFFEE achieves full independence from form compilers (and so relives the implementation burden) by providing a transformation to avoid the execution of such useless flops. A challenge for this transformation is restructuring the iteration spaces while preserving the effectiveness of low level optimizations, especially (auto-)vectorization.

Consider a set of tabulated basis functions with quadrature points along rows and functions along columns. For example, \texttt{A[i,j]} provides the value of the \texttt{j-th} basis function at quadrature point \texttt{i}. In~\citeN{quadrature1}, a technique to avoid iteration over zero-valued columns based on the use of indirection arrays (e.g. \texttt{A[B[i]]}, in which \texttt{A} is a tabulated basis function and \texttt{B} a map from loop iterations to non-zero columns in A) was proposed. Our approach aims to avoid such indirection arrays in the generated code. This is because we want to avoid non-contiguous memory loads and stores, which can nullify the benefits of vectorization. 

The idea is that if the dimension along which vectorization is performed (typically the innermost) has a contiguous slice of zeros, but that slice is smaller than the vector length, then we do nothing (i.e., the loop nest is not transformed). Otherwise, we restructure the iteration space. This has several non-trivial implications. The most notable one is memory offsetting (e.g., \texttt{A[i+m,j+n]}), which dramatically enters in conflict with padding and data alignment. We use heuristics to retain the positive effects of both and to ensure correctness. Details are, however, beyond the scope of this paper. 

The implementation is based on symbolic execution: the loop nests are traversed and for each statement encountered the location of zeros in each of the involved symbols is tracked. Arithmetic operators have a different impact on tracking. For example, multiplication requires computing the set intersection of the zero-valued slices (for each loop dimension), whereas addition requires computing the set union.

\section{Performance Evaluation}
\label{sec:perf-results}

\subsection{Experimental setup}

Experiments were run on a single core of an Intel I7-2600 (Sandy Bridge) CPU, running at 3.4GHz, 32KB L1 cache (private), 256KB L2 cache (private) and 8MB L3 cache (shared). The Intel Turbo Boost and Intel Speed Step technologies were disabled. The Intel \texttt{icc 15.2} compiler was used. The compilation flags used were \texttt{-O3, -xHost, -ip}.

We analyze the runtime performance of four real-world bilinear forms of increasing complexity, which comprise the differential operators that are most common in finite element methods. In particular, we study the mass matrix (``\texttt{Mass}'') and the bilinear forms arising in a Helmholtz equation (``\texttt{Helmholtz}''), in an elastic model (``\texttt{Elasticity}''), and in a hyperelastic model (``\texttt{Hyperelasticity}''). The complete specification of these forms is made publicly available\footnote{\url{https://github.com/firedrakeproject/firedrake-bench/blob/experiments/forms/firedrake_forms.py}}. 

We evaluate the speed-ups achieved by a wide variety of transformation systems over the ``original'' code produced by the FEniCS Form Compiler (i.e., no optimizations applied). We analyze the following transformation systems
\begin{itemize}
\item FEniCS Form Compiler: optimized quadrature mode (work presented in~\citeN{quadrature1}). Referred to as \texttt{quad} 
\item FEniCS Form Compiler: tensor mode (work presented in~\citeN{FFC-TC}). Referred to as \texttt{tens} 
\item FEniCS Form Compiler: automatic mode (choice between \texttt{tens} and \texttt{quad} driven by heuristic, detailed in~\citeN{Fenics} and summarized in Section~\ref{sec:qualitative}). Referred to as \texttt{auto} 
\item UFLACS: a novel back-end for the FEniCS Form Compiler (whose primary goals are improved code generation time and runtime). Referred to as \texttt{ufls} 
\item COFFEE: generalized loop-invariant code motion (work presented in~\citeN{Luporini}). Referred to as \texttt{cfO1} 
\item COFFEE: optimal loop nest synthesis plus symbolic execution for zero-elimination (work of this article). Referred to as \texttt{cfO2} 
\end{itemize}

The values that we report are the average of three runs with ``warm cache'' (no code generation time, no compilation time). They include the cost of local assembly as well as the cost of matrix insertion. However, the unstructured mesh used for the simulations (details below) was chosen small enough to fit the L3 cache of the CPU so as to minimize the ``noise'' due to operations outside of the element matrix evaluation. 

For a fair comparison, small patches (publicly available) were written to run \textit{all} simulations through Firedrake. This means the costs of matrix insertion and mesh iteration are identical in all variants. Our patches make UFLACS and the FEniCS Form Compiler's optimization systems generate code suitable for Firedrake, which employs a data storage layout different than that of FEniCS (e.g., array of pointers instead of pointer to pointers).

In Section~\ref{sec:mem-const}, we discussed the importance of memory constraints. We then define $T_H$ as the maximum amount of space that temporaries due to code motion can take. We set $T_H = L2_{size}$, that is, the size of the processor L2 cache (the last level of private cache). We recall that exceeding this threshold prevents the application of pre-evaluation. In our experiments, this happened in some circumstances. In such cases, experiments were repeated with $T_H = L3_{size}$ to verify the hypotheses made in Section~\ref{sec:mem-const}. We later elaborate on this.

Following the methodology adopted in~\citeN{quadrature1}, we vary the following parameters:
\begin{itemize}
\item the polynomial order of test, trial, and coefficient (or ``pre-multiplying'') functions, $q \in \lbrace1, 2, 3, 4\rbrace$
\item the number of coefficient functions $nf \in \lbrace0, 1, 2, 3\rbrace$
\end{itemize}
While constants of our study are
\begin{itemize}
\item the space of test, trial, and coefficient functions: Lagrange
\item the mesh: tetrahedral with a total of 4374 elements
\item exact numerical quadrature (we employ the same scheme used in~\citeN{quadrature1}, based on the Gauss-Legendre-Jacobi rule)
\end{itemize}

\subsection{Performance results}
\label{sec:perf-results-forms}

We report the results of our experiments in Figures~\ref{fig:mass},~\ref{fig:helmholtz},~\ref{fig:elasticity}, and~\ref{fig:hyperelasticity} as three-dimensional plots. The axes represent $q$, $nf$, and code transformation system. We show one subplot for each problem instance $\langle form, nf, q\rangle$, with the code transformation system varying within each subplot. The best variant for each problem instance is given by the tallest bar, which indicates the maximum speed-up over non-transformed code. We note that if a bar or a subplot are missing, then the form compiler failed at generating code because of either exceeding the system memory limit or unable to handle the form. 

The rest of the section is structured as follows: we provide insights about the main message of the experimentation; we comment on the impact of autovectorization; we explain in detail, individually for each form, the performance results obtained.

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/mass}}
\caption{Performance evaluation for the \textit{mass} matrix. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:mass}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/helmholtz}}
\caption{Performance evaluation for the bilinear form of a \textit{Helmholtz} equation. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:helmholtz}
\end{figure}

\begin{figure}
\includegraphics[scale=0.77]{perf-results/elasticity}
\caption{Performance evaluation for the bilinear form arising in an \textit{elastic} model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:elasticity}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/hyperelasticity}}
\caption{Performance evaluation for the bilinear form arising in a \textit{hyperelastic} model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:hyperelasticity}
\end{figure}


\paragraph{High level view}
The main observation is that our transformation strategy does not always guarantee minimum execution time. In particular, 5$\%$ of the test cases (3 out of 56, without counting marginal differences) show that \texttt{cfO2} was not optimal in terms of runtime. The most significant of such test cases is the elastic model with $[q=4, nf=0]$. There are two reasons for this. Firstly, low level optimization can have a significant impact on actual performance. For example, the aggressive loop unrolling in \texttt{tens} eliminates operations on zeros and reduces the working set size by not storing entire temporaries; on the other hand, preserving the loop structure can maximize the chances of autovectorization. Secondly, memory constraints are critical, particularly the transformation strategy adpopted when exceeding $T_H$. We will later thoroughly elaborate on all these aspects.

\paragraph{Autovectorization}
The discretizations employed result in inner loops and basis function tables of size multiple of the machine vector length. This, combined with the chosen compilation flags, promotes autovectorization in the majority of code variants. An exception is \texttt{quad} due to the presence of indirection arrays in the generated code. In \texttt{tens}, loop nests are fully unrolled, so the standard loop vectorization is not feasible; manual inspection of the compiled code suggests, however, that block vectorization (\cite{SLP-vect}) is often triggered. In \texttt{ufls}, \texttt{cfO1}, and \texttt{cfO2} the iteration spaces have similar structure (there are a few exceptions in \texttt{cfO2} due to zero-elimination), with loop vectorization being regularly applied, as far as we could evince from compiler reports and manual inspection of assembly code.

\paragraph{Mass}
We start with the simplest of the bilinear forms investigated, the mass matrix. Results are in Figure~\ref{fig:mass}. We first notice that the lack of improvements when $q=1$ is due to the fact that matrix insertion outweighs local assembly. As $q \geq 2$, \texttt{cfO2} generally shows the highest speed-ups. It is worth noting how \texttt{auto} does not always select the fastest implementation: \texttt{auto} always opts for \texttt{tens}, while as $nf \geq 2$ \texttt{quad} would tend to be preferable. On the other hand, \texttt{cfO2} always makes the optimal decision about whether applying pre-evaluation or not.  

\paragraph{Helmholtz}
As happened with the mass matrix problem, when $q=1$ matrix insertion still hides the cost of local assembly. For $q \geq 2$, the general trend is that \texttt{cfO2} outperforms the competitors. In particular, with
\begin{itemize}
\item $nf=0$, the adoption of pre-evaluation by \texttt{cfO2} results in increasingly notable speed-ups over \texttt{cfO1}, as $q$ increases; \texttt{tens} is comparable to \texttt{cfO2}, with \texttt{auto} making the right choice. 
\item $nf=1$, \texttt{auto} picks \texttt{tens}; the choice is however sub-optimal when $q=3$ and $q=4$. This can indirectly be inferred from the large gap between \texttt{cfO1/cfO2} and \texttt{tens/auto}: \texttt{cfO2} applies sharing elimination, but it avoids pre-evaluation. 
\item $nf=2$ and $nf=3$, \texttt{auto} reverts to \texttt{quad}, which would theoretically be the right choice (the flop count is much lower than in \texttt{tens} or what would be produced by pre-evaluation); however, the generated code suffers from the presence of indirection arrays, which break autovectorization and ``traditional'' code motion.
\end{itemize}

The sporadic slow-downs or only marginal improvements exhibited by \texttt{ufls} are imputable to the presence of sharing.

An interesting experiment we performed was relaxing the memory threshold by setting it to $T_H = L3_{size}$. We found that this makes \texttt{cfO2} generally slower as $nf \geq 2$, with a maximum slow-down of 2.16$\times$ with $\langle nf=2, q=2\rangle$. The effects of not having a sensible threshold could even be worse in parallel runs, since the L3 cache is shared by the cores.

\paragraph{Elasticity}
The results for the elastic model are displayed in Figure~\ref{fig:elasticity}. The main observation is that \texttt{cfO2} never triggers pre-evaluation, although in some occasions it should. To clarify this, consider the test case $\langle nf=0, q=2 \rangle$, in which \texttt{tens/auto} show a considerable speed-up over \texttt{cfO2}. \texttt{cfO2} finds pre-evaluation profitable -- that is, actually capable of reducing the operation count -- although it does not apply it because otherwise $T_H$ would be exceeded. However, running the same experiments with $T_H = L3_{size}$ resulted in a dramatic improvement, even higher than that of \texttt{tens}. Our explanation is that despite exceeding $T_H$ by roughly 40$\%$, the save in operation count is so large (5$\times$ in this problem) that pre-evaluation would anyway be the optimal choice. This suggests that our model could be refined to handle the cases in which there is a significant gap between potential cache misses and save in flops.

We also note that:
\begin{itemize}
\item the differences between \texttt{cfO2} and \texttt{cfO1} are due to systematic sharing elimination and the use of symbolic execution to avoid iteration over the zero-valued regions in the basis function tables
\item when $nf=1$, \texttt{auto} prefers \texttt{tens} to \texttt{quad}, which leads to sub-optimal operation counts and execution times
\item \texttt{ufls} generally shows better runtime behaviour than \texttt{quad} and \texttt{tens}. This is due to multiple facts, including avoidance of indirection arrays, preservation of loop structure, a more effective code motion.
\end{itemize}

\paragraph{Hyperelasticity}
In the experiments on the hyperelastic model, shown in Figure~\ref{fig:hyperelasticity}, \texttt{cfO2} exhibits the largest gains out of all problem instances considered in this paper. This is a positive aspect: it means that our transformation algorithm scales with form complexity. The fact that all code transformation systems (apart from \texttt{tens}) show quite significant speed-ups suggests several points. Firstly, the baseline is highly inefficient: with forms as complex as in this hyperelastic model, a trivial translation of integration routines into code should always be avoided since even one of the best general-purpose compilers available (Intel's on an Intel platform at maximum optimization level) is not capable of exploiting the structure inherent in the mathematical expressions generated. Secondly, the code motion strategy really makes a considerable impact. The sharing elimination performed by \texttt{cfO2} in each level of the loop nest ensures a critical reduction in operation count, which results is better execution times. In particular, at higher order, the main difference between \texttt{ufls} and \texttt{cfO2} is due to the application of this transformation to the multilinear loop nest. Clearly, the operation count increases with $q$, and so do the speed-ups. 



\section{Conclusions}
\label{sec:conclusions}
With this research we have set the foundation of optimal finite element integration. We have developed theory and implemented an automated system capable of applying it. The automated system, COFFEE, is integrated in Firedrake, a real-world framework for writing finite element methods. We believe the results are extremely positive. An open problem is understanding how to optimally handle non-linear loop nests. A second open problem is extending our methodology to classes of loops arising in spectral methods; here, the interaction with low level optimization will probably become stronger due to the typically larger working sets deriving from the use of high order function spaces. Lastly, we recall our work is publicly available and is already in use in the latest version of the Firedrake framework.

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{biblio}


\medskip

\end{document}
