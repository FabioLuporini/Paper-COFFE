% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{tabulary}
\usepackage{fancybox}
\usepackage{enumitem}

% font in listings
\usepackage{courier}

\makeatletter
\newenvironment{CenteredBox}{% 
\begin{Sbox}}{% Save the content in a box
\end{Sbox}\centerline{\parbox{\wd\@Sbox}{\TheSbox}}}% And output it centered
\makeatother

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{lineno}
\usepackage{xfrac}

% Theorems, definitions, lemmas, etc
\newtheorem{Def}{Definition}
\newtheorem{Prop}{Proposition}
\newtheorem{Algo}{Algorithm}
\newtheorem{Const}{Constraint}
\newtheorem{Strategy}{Strategy}
\newtheorem*{Rem*}{Remark}

\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}

% line over text in non math mode
\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\usepackage{listings}
\lstset{language=C, breaklines=true, mathescape,
			 morekeywords={each,min,not}}

\usepackage[cmex10]{amsmath}
\usepackage{url}

\DeclareMathOperator*{\argmin}{arg\,min}

% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}

% Document starts
\begin{document}

% Page heads
\markboth{F. Luporini et al.}{On Optimality of Finite Element Integration}

% Title portion
\title{On Optimality of Finite Element Integration}
\author{Fabio Luporini
\affil{Imperial College London}
David A. Ham
\affil{Imperial College London}
Paul H. J. Kelly
\affil{Imperial College London}}

\begin{abstract}
We tackle the problem of automatically generating optimal finite element integration routines given a high level specification of arbitrary multilinear forms. Optimality and quasi-optimality are defined in terms of floating point operations given a memory bound. We provide an approach for exploring the space of legal transformations, which systematically reaches quasi-optimality. The conditions under which optimality is achieved are also discussed. A theoretical analysis and extensive experimentation, which shows significant performance improvements over several state-of-the-art code generation systems, validate the approach.
\end{abstract}

\category{G.1.8}{Numerical Analysis}{Partial Differential Equations -
  Finite element methods}

\category{G.4}{Mathematical Software}{Parallel and vector implementations}

\terms{Design, Performance}

\keywords{Finite element integration, local assembly, compilers, performance optimization}

\acmformat{Fabio Luporini, David A. Ham, and Paul H. J. Kelly, 2015. On Optimality of Finite Element Integration.}

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT: Full first names whenever they are known, surname last,
% followed by a period.  In the case of two authors, 'and' is placed
% between them.  In the case of three or more authors, the serial
% comma is used, that is, all author names except the last one but
% including the penultimate author's name are followed by a comma, and
% then 'and' is placed before the final author's name.  If only first
% and middle initials are known, then each initial is followed by a
% period and they are separated by a space.  The remaining information
% (journal title, volume, article number, date, etc.) is
% 'auto-generated'.

\begin{bottomstuff}

This research is partly funded by the MAPDES project, by the
Department of Computing at Imperial College London, by EPSRC through
grants EP/I00677X/1, EP/I006761/1, and EP/L000407/1, by NERC grants
NE/K008951/1 and NE/K006789/1, by the U.S.  National Science
Foundation through grants 0811457 and 0926687, by the U.S. Army
through contract W911NF-10-1-000, and by a HiPEAC collaboration
grant. The authors would like to thank Mr. Andrew T.T. McRae,
Dr. Lawrence Mitchell, and Dr. Francis Russell for their invaluable
suggestions and their contribution to the Firedrake project.

Author's addresses: Fabio Luporini $\&$ Paul H. J. Kelly, Department of Computing,
Imperial College London; David A. Ham, Department of Computing and
Department of Mathematics, Imperial College London; 
\end{bottomstuff}

\maketitle


\section{Introduction}

The need for rapid implementation of high performance, robust, and portable finite element methods has led to approaches based on automated code generation. This has been proven successful in the context of the FEniCS (\cite{Fenics}) and Firedrake (\cite{firedrake-paper}) projects. In these frameworks, the weak variational form of a problem is expressed at high level by means of a domain-specific language. The mathematical specification is manipulated by a form compiler that generates a representation of assembly operators. By applying these operators to an element in the discretized domain, a local matrix and a local vector, which represent the contributions of that element to the equation approximated solution, are computed. The code for assembly operators must be carefully optimized: as the complexity of a variational form increases, in terms of number of derivatives, pre-multiplying functions, or polynomial order of the chosen function spaces, the operation count increases, with the result that assembly often accounts for a significant fraction of the overall runtime. 

As demonstrated by the substantial body of research on the topic, automating the generation of such high performance implementations poses several challenges. This is a result of the complexity inherent in the mathematical expressions involved in the numerical integration, which varies from problem to problem, and the particular structure of the loop nests enclosing the integrals. General-purpose compilers, such as \emph{GNU's} and \emph{Intel's}, fail at exploiting the structure inherent in the expressions, thus producing sub-optimal code (i.e., code which performs more floating-point operations, or ``flops'', than necessary; we show this is Section~\ref{sec:perf-results}). Research compilers, for instance those based on polyhedral analysis of loop nests such as PLUTO (\cite{PLUTO}), focus on parallelization and optimization for cache locality, treating issues orthogonal to the question of minimising flops. The lack of suitable third-party tools has led to the development of a number of domain-specific code transformation (or synthesizer) systems. In~\citeN{quadrature1}, it is shown how automated code generation can be leveraged to introduce optimizations that a user should not be expected to write ``by hand''. In~\citeN{FFC-TC} and~\citeN{Francis}, mathematical reformulations of finite element integration are studied with the aim of minimizing the operation count. In~\citeN{Luporini}, the effects and the interplay of generalized code motion and a set of low level optimizations are analysed. It is also worth mentioning an on-going effort to produce a new form compiler, called UFLACS (\cite{Uflacs}), which adds to the already abundant set of code transformation systems for assembly operators. The performance evaluation in Section~\ref{sec:perf-results} includes most of these optimization systems.

However, in spite of such a considerable research effort, still there is no answer to one fundamental question: can we automatically generate an implementation of a form which is optimal in the number of flops executed? In this paper, we formulate an approach that solves this problem for a particular class of forms and provides very good approximations in all other cases (we will define ``quasi-optimality'', which relates operation count with inner loops). Summarizing, our contributions are as follows:

\begin{itemize}
\item We formalize finite element integration loop nests and we build the space of legal transformations impacting their operation count.
\item We provide an algorithm to select points in the transformation space. The algorithm uses a cost model to: (i) understand whether a transformation reduces or increases the operation count; (ii) choose between different (non-composable) transformations.
\item We demonstrate that our algorithm systematically leads to quasi-optimality. We also explain under what conditions of the input problem optimality is achieved.
\item We integrate our approach with a compiler, COFFEE\footnote{COFFEE stands for COmpiler For Fast Expression Evaluation. The compiler is open-source and available at \url{https://github.com/coneoproject/COFFEE}}, which is in use in the Firedrake framework.
\item We experimentally evaluate using a broader suite of forms, discretizations, and code generation systems than has been used in prior research. This is essential to demonstrate that our optimality model holds in practice.
\end{itemize}

In addition, in order to place COFFEE on the same level as other code generation systems from the viewpoint of low level optimization (which is essential for a fair performance comparison)

\begin{itemize}
\item We introduce an engine based on symbolic execution that allows skipping irrelevant floating point operations (e.g., those involving zero-valued quantities).
\end{itemize}


\section{Preliminaries}
\label{sec:background}
We review finite element integration using the same notation and examples adopted in~\citeN{quadrature1} and~\citeN{Francis}. 

We consider the weak formulation of a linear variational problem
\begin{equation}
\begin{split}
\text{Find}\ u \in U\ \text{such that} \\
a(u, v) = L(v), \forall v \in V
\end{split}
\end{equation}
where $a$ and $L$ are, respectively, a bilinear and a linear form. The set of \textit{trial} functions $U$ and the set of \textit{test} functions $V$ are discrete function spaces. For simplicity, we assume $U = V$. Let $\lbrace \phi_i \rbrace$ be the set of basis functions spanning $U$. The unknown solution $u$ can be approximated as a linear combination of the basis functions $\lbrace \phi_i \rbrace$. From the solution of the following linear system it is possible to determine a set of coefficients to express $u$:
\begin{equation}
Au = b
\end{equation}
in which $A$ and $b$ discretize $a$ and $L$ respectively:
\begin{equation}
\centering
\begin{split}
A_{ij} = a(\phi_i(x), \phi_j(x)) \\
b_i = L(\phi_i(x))
\end{split}
\end{equation}
The matrix $A$ and the vector $b$ are ``assembled'' and subsequently used to solve the linear system through (typically) an iterative method.

We focus on the assembly phase, which is often characterized as a two-step procedure: \textit{local} and \textit{global} assembly. Local assembly is the subject of this article. It consists of computing the contributions of a single element in the discretized domain to the equation approximated solution. In global assembly, such local contributions are ``coupled'' by suitably inserting them into $A$ and $b$. 

We illustrate local assembly in a concrete example, the evaluation of the local element matrix for a Laplacian operator. Consider the weighted Laplace equation
\begin{equation}
- \nabla \cdot (w \nabla u) = 0
\end{equation}
in which $u$ is unknown, while $w$ is prescribed. The bilinear form associated with the weak variational form of the equation is:
\begin{equation}
a(v, u) = \int_\Omega w \nabla v \cdot \nabla u\ \mathrm{d}x
\end{equation}
The domain $\Omega$ of the equation is partitioned into a set of cells (elements) $T$ such that $\bigcup T = \Omega$ and $\bigcap T = \emptyset$. By defining $\lbrace \phi_i^K \rbrace$ as the set of local basis functions spanning $U$ on the element $K$, we can express the local element matrix as
\begin{equation}
\label{stiffness}
A_{ij}^K = \int_K w \nabla \phi_i^K \cdot \nabla \phi_j^K\ \mathrm{d}x
\end{equation}
The local element vector $L$ can be determined in an analogous way. 

\subsection{Monomials}
\label{sec:monomials}
It has been shown (e.g., in~\citeN{Kirby:TC}) that local element tensors can be expressed as a sum of integrals over $K$, each integral being the product of derivatives of functions from sets of discrete spaces and, possibly, functions of some spatially varying coefficients. An integral of this form is called \textit{monomial}. 

%From the computational perspective, its evaluation is however less expensive than that of $A$.

\subsection{Quadrature mode}
Quadrature schemes are typically used to numerically evaluate $A_{ij}^K$. For convenience, a reference element $K_0$ and an affine mapping $F_K : K_0 \rightarrow K$ to any element $K \in T$ are introduced. This implies that a change of variables from reference coordinates $X_0$ to real coordinates $x = F_K (X_0)$ is necessary any time a new element is evaluated. The numerical integration routine based on quadrature over an element $K$ can be expressed as follows
\begin{equation}
\label{eq:quadrature}
A_{ij}^K = \sum_{q=1}^N \sum_{\alpha_3=1}^n \phi_{\alpha_3}(X^q)w_{\alpha_3} \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\beta=1}^d \frac{\partial X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial \phi_i^K(X^q)}{\partial X_{\alpha_1}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \frac{\partial \phi_j^K(X^q)}{\partial X_{\alpha_2}} det F_K' W^q
\end{equation}
where $N$ is the number of integration points, $W^q$ the quadrature weight at the integration point $X^q$, $d$ is the dimension of $\Omega$, $n$ the number of degrees of freedom associated to the local basis functions, and $det$ the determinant of the Jacobian matrix used for the aforementioned change of coordinates.  

% Magari la summation sui coefficients la sputo dentro? che dici?

\subsection{Tensor contraction mode}
\label{sec:tc}
Starting from Equation~\ref{eq:quadrature}, exploiting linearity, associativity and distributivity of the involved mathematical operators, we can rewrite the expression as
\begin{equation}
\label{eq:tensor}
A_{ij}^K = \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\alpha_3=1}^n det F_K' w_{\alpha_3} \sum_{\beta=1}^d \frac{X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \int_{K_0} \phi_{\alpha_3} \frac{\partial \phi_{i_1}}{\partial X_{\alpha_1}} \frac{\partial \phi_{i_2}}{\partial X_{\alpha_2}} dX.
\end{equation}
A generalization of this transformation has been proposed in~\cite{Kirby:TC}. Because of only involving reference element terms, the integral in the equation can be pre-evaluated and stored in temporary variables. The evaluation of the local tensor can then be abstracted as
\begin{equation}
A_{ij}^K = \sum_{\alpha} A_{i_1 i_2 \alpha}^0 G_{K}^\alpha
\end{equation}
in which the pre-evaluated ``reference tensor'' $A_{i_1 i_2 \alpha}$ and the cell-dependent ``geometry tensor'' $G_{K}^\alpha$ are exposed. 

\subsection{Qualitative comparison}
\label{sec:qualitative}
Depending on form and discretization, the relative performance of the two modes, in terms of the operation count, can vary quite dramatically. The presence of derivatives or coefficient functions in the input form tends to increase the size of the geometry tensor, making the traditional quadrature mode preferable for ``complex'' forms. On the other hand, speed-ups from adopting tensor mode can be significant in a wide class of forms in which the geometry tensor remains ``sufficiently small''. The discretization, particularly the relative polynomial order of trial, test, and coefficient functions, also plays a key role in the resulting operation count. 

These two modes have been implemented in the FEniCS Form Compiler (\cite{FFC-TC}). In this compiler, a heuristic is used to choose the most suitable mode for a given form. It consists of analysing each monomial in the form, counting the number of derivatives and coefficient functions, and checking if this number is greater than a constant found empirically (\cite{Fenics}). We will later comment on the efficacy of this approach (Section~\ref{sec:perf-results}). For the moment, we just recall that one of the goals of this research is to produce a system that goes beyond the dichotomy between quadrature and tensor modes. We will reason in terms of loop nests, code motion, and code pre-evaluation, searching the entire implementation space for an optimal synthesis.  

\section{Transformation Space}
\label{sec:optimal-impl}
In this section, we characterize optimality and quasi-optimality for finite element integration as well as the space of legal transformations that needs be explored to achieve them. How the exploration is performed is discussed in Section~\ref{sec:optimal-synthesis}. 

\subsection{Loop nests, expressions and optimality}
\label{sec:lnopt}
In order to make the article self-contained, we start with reviewing basic compiler terminology.

\begin{Def}[Perfect and imperfect loop nests]
A perfect loop nest is a loop whose body either 1) comprises only a sequence
of non-loop statements or 2) is itself a perfect loop nest. If this
condition does not hold, a loop nest is said to be imperfect. 
\end{Def}

\begin{Def}[Independent basic block]
An independent basic block is a sequence of statements such that no data
dependencies exist between statements in the block.
\end{Def}

We focus on perfect nests whose innermost loop body is an independent basic
block. A straightforward property of this class is that hoisting invariant
expressions from the innermost to any of the outer loops or the preheader
(i.e., the block that precedes the entry point of the nest) is always safe,
as long as any dependencies on loop indices are honored. We will make use of this property. The results of this section could also be generalized to larger classes of loop nests, in which basic block independence does not hold, although this would require refinements beyond the scope of this paper. 

By mapping mathematical properties to the loop nest level, we introduce the
concepts of a \textit{linear loop} and, more generally, a (perfect) multilinear loop nest.

\begin{Def}[Linear loop]
\label{def:linear-loop}
A loop $L$ defining the iteration space $I$ through the iteration variable $i$, or simply $L_i$, is linear if in its body
\begin{enumerate}
\item $i$ appears only as an array index, and
\item whenever an array $a$ is indexed by $i$ ($a[i]$), all expressions in which this appears are affine in $a$.
\end{enumerate}
\end{Def}

\begin{Def}[Multilinear loop nest]
\label{def:multi-linear-loop}
A multilinear loop nest of arity $n$ is a perfect nest composed of $n$ loops, in which all of the expressions appearing in the body of the innermost loop are affine in each loop $L_i$ separately.
\end{Def}

We will show that multilinear loop nests, which arise naturally when translating bilinear or linear forms into code, are important because they have a structure that we can take advantage of to synthesize quasi-optimal code.

We define two other classes of loops. 

\begin{Def}[Reduction loop]
\label{def:i-loop}
A loop $L_i$ is said to be a reduction loop if in its body
\begin{enumerate}
\item $i$ appears only as an array index, and
\item for each augmented assignment statement $S$ (e.g., an increment), arrays indexed by $i$ appear only on the right hand side of $S$.
\end{enumerate}
\end{Def}

\begin{Def}[Order-free loop]
\label{def:e-loop}
A loop $L_i$ is said to be an order-free loop if its iterations can be executed in any arbitrary order. 
\end{Def}

\begin{figure}\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest.code}
\end{CenteredBox}\caption{The loop nest implementing a generic bilinear form.}\label{code:loopnest}\end{figure}

Consider Equation~\ref{eq:quadrature} and the (abstract) loop nest implementing it illustrated in Figure~\ref{code:loopnest}. The imperfect nest $\Lambda=[L_e, L_i, L_j, L_k]$ comprises an order-free loop $L_e$ (over elements in the mesh), a reduction loop $L_i$ (performing numerical integration), and a multilinear loop nest $[L_j, L_k]$ (over test and trial functions). In the body of $L_k$, one or more statements evaluate the local tensor for the element $e$. Expressions (right hand side of a statement) result from the translation of a form in high level matrix notation into code. In particular, $m$ is the number of ``terms'' (a monomial can be implemented by one or more terms), $\alpha_{eij}$ ($\beta_{eik}$) represents the product of a coefficient function (e.g., the inverse Jacobian matrix for the change of coordinates) with test (trial) functions, and $\sigma_{ei}$ is a function of coefficients and geometry. We do not pose any restrictions on function spaces (e.g., scalar- or vector-valued), coefficients (e.g., linear or non-linear), differential and vector operators, so $\sigma_{ei}$ can be arbitrarily complex. We say that such an expression is in \textit{normal form}, because the algebraic structure of a variational form is intact (e.g., products have not been expanded yet, distinct monomials can still be identified, etc.). This bring us to formalize the class of loop nests that we aim to optimize.

%TODO is "not expanded yet == still contracted" ?

\begin{Def}[Finite element integration loop nest]
\label{def:fem-loopnest}
A finite element integration loop nest is a loop nest in which the following appear, in order: an imperfect order-free loop, an imperfect (perfect only in some special cases), linear or non-linear reduction loop, and a multilinear loop nest whose body is an independent basic block in which expressions are in normal form.
\end{Def}

We then characterize optimality for a finite element integration loop nest as follows.

\begin{Def}[Optimality of a loop nest]
\label{def:mln-optimality}
Let $\Lambda$ be a generic loop nest, and let $\Gamma$ be a transformation function $\Gamma : \Lambda \rightarrow \Lambda'$ such that $\Lambda'$ is semantically equivalent to $\Lambda$ (possibly, $\Lambda' = \Lambda$). We say that $\Lambda' = \Gamma (\Lambda)$ is an optimal synthesis of $\Lambda$ if the total number of operations (additions, products) performed to evaluate the result is minimal.
\end{Def}

The concept of quasi-optimality, which relies on the particular class of \textit{flop-decreasing} transformations, is also introduced.

\begin{Def}[Flop-decreasing transformation]
A sequence of transformations reducing the operation count is called flop-decreasing.
\end{Def}

\begin{Def}[Quasi-optimality of a loop nest]
\label{def:mln-quasi-optimality}
Given $\Lambda$, $\Lambda'$ and $\Gamma$ as in Definition~\ref{def:mln-optimality}, we say that $\Lambda' = \Gamma (\Lambda)$ is a quasi-optimal synthesis of $\Lambda$ if:
\begin{itemize}
\item the number of operations (additions, products) in the innermost loops performed to evaluate the result is minimal, and
\item $\Gamma$ is expressed as composition of flop-decreasing transformations.
\end{itemize}
\end{Def}

The restriction to flop-decreasing transformations aims to exclude those ``deceptive'' solutions that, to achieve flop-optimal innermost loops, would rearrange the computation at the level of the outer loops causing, in fact, a ``global'' increase in operation count. 

We also observe that Definitions~\ref{def:mln-optimality} and~\ref{def:mln-quasi-optimality} do not take into account memory requirements. If the loop nest were memory-bound -- the ratio of operations to bytes transferred from memory to the CPU being too low -- then speaking of optimality would clearly make no sense. Henceforth we assume to operate in a CPU-bound regime, in which arithmetic-intensive expressions need be evaluated. In the context of finite elements, this is often true for more complex multilinear forms and/or higher order elements. 

Achieving optimality in polynomial time is not generally feasible, since the $\sigma_{ei}$ sub-expressions can be arbitrarily unstructured. On the other hand, multilinearity suggests a certain degree of regularity for $\alpha_{eij}$ and $\beta_{eik}$. In the following sections, we will elaborate on these observations and formulate an approach that achieves: (i) at least quasi-optimality in all cases; (ii) optimality whenever the monomials are ``sufficiently structured''. To this purpose, we will construct:
\begin{itemize}
\item the space of legal transformations impacting the operation count (Sections~\ref{sec:sharing-elimination} -- \ref{sec:completeness})
\item an algorithm to select points in the transformation space (Section~\ref{sec:optimal-synthesis})
\end{itemize}

\subsection{Sharing elimination}
\label{sec:sharing-elimination}
We start with introducing the fundamental notion of sharing.

\begin{Def}[Sharing]
A statement within a loop nest $\Lambda$ presents sharing if at least one of the following conditions hold:
\begin{description}
\item[Spatial sharing] There are at least two symbolically identical sub-expressions
\item[Temporal sharing] There is at least one non-trivial sub-expression (e.g., an addition or a product) that is redundantly executed because it is independent of $\lbrace L_{i_1}, L_{i_1}, ...L_{i_n} \rbrace \subset \Lambda$.
\end{description}
\end{Def}

To illustrate the definition, we show in Figure~\ref{code:multi_loopnest} how sharing evolves as factorization and code motion are applied to a trivial multilinear loop nest. In the original loop nest (Figure~\ref{code:multi_loopnest_a}), spatial sharing is induced by the symbol $b_j$. Factorization eliminates spatial sharing and creates temporal sharing (Figure~\ref{code:multi_loopnest_b}). Finally, generalized code motion \cite{Luporini}, which seeks sub-expressions that are redundantly executed by at least one loop in the nest\footnote{Traditional loop-invariant code motion, which is commonly applied by general-purpose compilers, only checks invariance with respect to the innermost loop.}, leads to optimality (Figure~\ref{code:multi_loopnest_c}). 

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 13pt \subfigure[With spatial sharing]{\label{code:multi_loopnest_a}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest.code}}}
~~~~~
{\subfigcapskip = 13pt \subfigure[With temporal sharing]{\label{code:multi_loopnest_b}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest_int.code}}}
~~~~~
{\subfigcapskip = 4pt \subfigure[Optimal form]{\label{code:multi_loopnest_c}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest_opt.code}}}
\end{CenteredBox}\caption{Reducing a simple multilinear loop nest to optimal form.}\label{code:multi_loopnest}\end{figure}

In this section, we study \textit{sharing elimination}, a transformation that aims to reduce the operation count by removing sharing through the application of expansion, factorization, and generalized code motion. If the objective were reaching optimality and the expressions lacked structure, a transformation of this sort would require solving a large combinatorial problem -- for instance to evaluate the impact of all possible factorizations. Our sharing elimination strategy, instead, exploits the structure inherent in finite element integration expressions to guarantee quasi-optimality\footnote{This requires coordination with other transformations, as discussed in the following sections. For the moment -- and for ease of exposition -- we neglect this aspect.}, with optimality being achieved if stronger preconditions hold. Setting quasi-optimality, instead of optimality, as primary goal is essential to produce simple and computationally efficient algorithms -- two necessary conditions for integration with a compiler. 

%Section~\ref{sec:se-rln} discusses structural and algebraic properties characterizing our expressions. Section~\ref{sec:se-algo} presents the sharing elimination algorithm. Examples are provided in Section~\ref{sec:se-examples}. We recall that we are assuming a generic finite element integration loop nest $\Lambda=[L_e, L_i, L_j, L_k]$ with expressions in normal form.

\subsubsection{Identification and exploitation of structure}
\label{sec:se-rln}
Finite element expressions can be seen as composition of operations between tensors. Often, the optimal implementation strategy for these operations is to be determined out of two alternatives. For instance, consider $J^{-1} \nabla v \cdot J^{-1} \nabla v$, with $J^{-1}$ being the inverse Jacobian matrix for the change of coordinates and $v$ a generic two-dimensional vector. The tensor operation will reduce to the scalar expression $(a v^0_i + b v^1_i) (a v^0_i + b v^1_i) + ...$, in which $v^0_i$ and $v^1_i$ represent components of $v$ that depend on $L_i$. To minimize the operation count for expressions of this kind, we have two options.
\begin{Strategy}
\label{strategy:i}
Eliminating temporal sharing through generalized code motion.
\end{Strategy}
\begin{Strategy}
\label{strategy:ii}
Eliminating spatial sharing first -- through product expansion and factorization -- and temporal sharing afterwards, again through generalized code motion.
\end{Strategy}
In the current example, we observe that, depending on the size of $L_i$, applying Strategy~\ref{strategy:ii} could reduce the operation count since the expression would be recast as $v^0_i v^0_i a a + v^0_i v^1_i (ab + ab) + v^1_i v^1_i c c + ...$ and some hoistable sub-expressions would be exposed. On the other hand, Strategy~\ref{strategy:i} would have no effect as $v$ only depends on a single loop, $L_i$. In general, the choice between the two strategies depends on multiple factors: the loop(s) size, the increase in operation count due to expansion (in Strategy~\ref{strategy:ii}), and the gain in code motion. A second application of Strategy~\ref{strategy:ii} was provided in Figure~\ref{code:multi_loopnest}. These examples motivate the introduction of a particular class of expressions, for which the two strategies assume notable importance.
\begin{Def}[Structured expression]
\label{def:struct-expr}
We say that ``an expression is structured along a loop nest $\Lambda$'' if and only if in order to eliminate the spatial sharing induced by any $s_{\Lambda}$ -- a generic symbol depending on at least one loop in $\Lambda$ -- it is sufficient to factorize all occurrences of $s_{\Lambda}$ in the expression.
\end{Def}
\begin{Prop}
\label{prop:multi-struct}
An expression along a multilinear loop nest is structured.
\end{Prop}
\begin{proof}
This descends directly from Definition~\ref{def:linear-loop} and Definition~\ref{def:multi-linear-loop}, which essentially restrict the number of occurrences of a symbol $s_{\Lambda}$ in a summand to at most 1.
\end{proof}
If $\Lambda$ were an arbitrary loop nest, a given symbol $s_{\Lambda}$ could appear everywhere (e.g., $n$ times in a summand and $m$ times in another summand with $n \neq m$, as argument of a higher level function, in the denominator of a division), thus posing the challenge of finding the factorization that maximizes temporal sharing. If $\Lambda$ is instead a finite element integration loop nest, thanks to Proposition~\ref{prop:multi-struct} the space of flop-decreasing transformations is constructed by ``composition'' of Strategy~\ref{strategy:i} and Strategy~\ref{strategy:ii}, as illustrated in Algorithm~\ref{algo:sharing-elimination}.

Finally, we observe that the $\sigma_{ei}$ sub-expressions can sometimes be considered ``weakly structured''. This happens when a relaxed version of Definition~\ref{def:struct-expr}, in which the factorization of $s_{\Lambda}$ only ``minimizes'' (rather than ``eliminates'') spatial sharing, applies (for instance, in the complex hyperelastic model analyzed in Section~\ref{sec:perf-results}). Weak structure will be exploited by Algorithm~\ref{algo:sharing-elimination} in the attempt to achieve optimality.

%.the application of Strategy~\ref{strategy:i} or Strategy~\ref{strategy:ii} to all sub-expressions in which $s_{\Lambda}$ appears
% the spatial sharing induced by $s_{\Lambda}$ -- a generic symbol depending on at least one loop in $\Lambda$ -- can be eliminated through factorization of all instances of $s_{\Lambda}$.

\subsubsection{Algorithm}
\label{sec:se-algo}
Algorithm~\ref{algo:sharing-elimination} describes sharing elimination assuming as input a tree representation of the loop nest. It makes use of the following notation and terminology:
\begin{itemize}
\item $|.|$: a generic ``sizeof'' operator (e.g., cardinality of a set, extent of a loop).
\item \textit{multilinear operand}: any $\alpha_{eij}$ or $\beta_{eik}$ in the input expression.
\item \textit{multilinear symbol}: a symbol appearing within a multilinear operand depending on $L_j$ or $L_k$ (e.g., test functions, first order derivatives of test functions, etc.).
\end{itemize}
Examples will be provided in Section~\ref{sec:se-examples}.

\begin{Algo}[Sharing elimination]
\label{algo:sharing-elimination}
\normalfont
The algorithm has three main phases: initialization (step 1); scheduling of multilinear operands for reaching quasi-optimality (steps 2-6); scheduling of all other sub-expressions (step 7).
\begin{enumerate}
\item Perform a depth-first visit of the expression tree to group identical terms within the same sub-expression. \\ 
\textit{Note: for example, an expression $(a + a + b + c + a)$ would be transformed into $(3a + b + c)$}.

\item Perform a depth-first visit of the expression tree to collect and partition multilinear operands into disjoint sets, $\mathbb{P} = \lbrace P^1, ..., P^{p}\rbrace$. $\mathbb{P}$ is such that all multilinear operands in $P$ share the same set of multilinear symbols $S_P$, whereas there is no sharing across different partitions.

\item For all multilinear operands in $P \in \mathbb{P}$ such that $|P| \leq |S_P|$, apply Strategy~\ref{strategy:i}. \\
\textit{Note: $|P|$ and $|S_P|$ represent the number of products in the innermost loop induced by $P$ if, respectively, Strategy~\ref{strategy:i} or Strategy~\ref{strategy:ii} were applied. This is a consequence of Proposition~\ref{prop:multi-struct}.}

\item Build the \textit{sharing graph} $\mathbb{G} = (\mathbb{S}, \mathbb{E})$, with $S^i \in \mathbb{S}$ representing a multilinear symbol or a temporary produced at step (3). An edge $(S^i$, $S^j)$ indicates that a product $S^i S^j$ would appear if the sub-expressions including $S^i$ and $S^j$ were expanded. Let $\mathbb{T}$ be the subset of vertices with at most one incident edge, or ``terminals''. 

\item Remove $e = (S^i, S^j) \in \mathbb{E}$ if applying Strategy~\ref{strategy:ii} would cause an increase in operation count (i.e., the elimination of temporal sharing after factorization of $S^i$ or $S^j$ would be outweighed by the expansion cost).\\
\textit{Note: this ensures the flop-decreasing nature of the transformations at step (6)}.\\
\textit{Note: this pass can be accomplished by visiting the expression tree and collecting several kinds of information -- for example, the number of products wrapping an instance of $S^i$ or $S^j$, from which the expansion cost will be derived.}


\item If $\mathbb{S} = \mathbb{T}$, jump to step (7). Otherwise apply, in order, the following rewrite rules (\textit{precondition}: \textit{action}):
\begin{enumerate}[label=(\Alph*)]
\item $\lbrace T^1, ..., T^n \rbrace \subset \mathbb{T}$ adjacent to $S$, $n > 1$: take $S$ and apply Strategy~\ref{strategy:ii}.\\
\textit{Note: this recasts the expression as $S (...T^1 + ... + ...T^n)$}.
\item Only one terminal $T$ adjacent to $S$: take an arbitrary non-terminal symbol $S^{nt}$ adjacent to $S$ and apply Strategy~\ref{strategy:ii}.\\
\textit{Note: this recasts the expression as $S (...T + ...S^{nt})$}.
\item There is no cyclic path $[S, W, ..., S]$: take $S$ and apply Strategy~\ref{strategy:ii}, similarly to (B).\\
\textit{Note: this recasts the expression as $S (...W + ...)$}.\\
\textit{Note: the idea is that the product ${S}\ {W}$ cannot be ``hidden within larger factorizations''. Further details will be provided in Section~\ref{sec:proof}}.
\item $[S, ..., V, ..., S]$ is a cycle, $\lbrace S^1, ..., S^n \rbrace$ adjacent to $S$: take $S$ and apply Strategy~\ref{strategy:ii}.\\
\textit{Note: this recasts the expression as $S (...S^1 + ...S^n)$}.
\end{enumerate}
Go back to step (2).

\item Perform a depth-first visit of the expression tree and, for each yet unhandled or hoisted expression, apply the most profitable between Strategy~\ref{strategy:i} and Strategy~\ref{strategy:ii}. \\
\textit{Note: this pass speculatively assumes that expressions are (weakly) structured along the reduction loop. If the assumption does not hold, the operation count will generally be sub-optimal because only a subset of factorizations and code motion opportunities may eventually be considered.}
\end{enumerate}
\end{Algo}

\begin{Rem*}
\normalfont
Although the primary goal of Algorithm~\ref{algo:sharing-elimination} is operation count minimization within the multilinear loop nest, the enforcement of flop-decreasing transformations (step 5) and the re-scheduling of sub-expressions within outer loops (step 7) also attempt to optimize the loop nest ``globally''. We will further elaborate this aspect in Section~\ref{sec:proof}.
\end{Rem*}


\subsubsection{Examples}
\label{sec:se-examples}
Consider again Figure~\ref{code:multi_loopnest_a}. We have $\mathbb{P} = \lbrace P^0, P^1, P^2\rbrace$, with $P^0 = \lbrace b_j\rbrace$, $P^1 = \lbrace c_i\rbrace$, and $P^2 = \lbrace d_i\rbrace$. For all $P^i$, we have $|P^i| = 1 = |S_{P^i}|$, although applying Strategy~\ref{strategy:i} in step 3 has no effect. The sharing graph is $\mathbb{G} = (\lbrace b_j, c_i, d_i \rbrace, \lbrace (b_j, c_i), (b_j, d_i) \rbrace$, and $\mathbb{T} = \lbrace c_i, d_i \rbrace$. Rewrite rule (4A) is hit, which leads to the code in Figure~\ref{code:multi_loopnest_c}.

In Figure~\ref{code:poisson}, Algorithm~\ref{algo:sharing-elimination} is executed in a realistic scenario, which originates from the bilinear form of a Poisson equation in two dimensions. We observe that $\mathbb{P} = \lbrace P^0, P^1\rbrace$, with $P^0 = \lbrace (z_0 a_{ik} + z_2 b_{ik}), (z_1 a_{ik} + z_3 b_{ik})\rbrace$ and $P^1 = \lbrace (z_0 a_{ij} + z_2 b_{ij}), (z_1 a_{ij} + z_3 b_{ij})\rbrace$. In addition, $|P^i| = |S_{P^i}| = 2$, so Strategy~\ref{strategy:i} is applied to both partitions (step 3). We then have (step 4) $\mathbb{G} = (\lbrace t^0, t^1, t^2, t^3 \rbrace, \emptyset)$, although no rewrite rule is applicable since $\mathbb{S} = \mathbb{T}$. 

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 7pt \subfigure[Normal form]{\label{code:poisson_a}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/poisson_pre_se.code}}}
~~~~~~~~~~
{\subfigcapskip = 7pt \subfigure[After sharing elimination]{\label{code:poisson_b}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/poisson_after_se.code}}}
\end{CenteredBox}\caption{Applying sharing elimination to the bilinear form arising from a Poisson equation in 2D. The operation counts are $E(f(z^0, z^1, ...) + IJK \cdot 18)$ (left) and $E(f(z^0, z^1, ...) + I(J \cdot 6 + K \cdot 9 + JK \cdot 4)$ (right), with $f(z^0, z^1, ...)$ representing the operation count for evaluating $z^0, z^1, ...$, and common sub-expressions being counted once. Note that the synthesis in Figure~\ref{code:poisson_b} reaches optimality apart from the pathological case  $I, J, K = 1$.}\label{code:poisson}\end{figure}

A much more complex example, using the hyperelastic model evaluated in Section~\ref{sec:perf-results}, is made available online\footnote{Sharing elimination example - application to a hyperelastic model: \url{https://gist.github.com/FabioLuporini/14e79457d6b15823c1cd}}.

\subsection{Monomial pre-evaluation}
\label{sec:pre-evaluation}
Sharing elimination uses three operators: expansion, factorization, and code motion. In this section, we discuss the role and legality of a fourth operator: reduction pre-evaluation. We will see that what makes this operator special is the fact that there exists a single point in the transformation space of a monomial (i.e., a specific factorization of test, trial, and coefficient functions) ensuring its correctness.

We start with an example. Consider again the loop nest and the expression in Figure~\ref{code:loopnest}. We pose the following question: are we able to identify sub-expressions for which the reduction induced by $L_i$ can be pre-evaluated, thus obtaining a decrease in operation count proportional to the size of $L_i$, $I$? The transformation we look for is exemplified in Figure~\ref{code:loopnest_rednored} with a simple loop nest. The reader may verify that a similar transformation is applicable to the example in Figure~\ref{code:poisson_a}.

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 19pt \subfigure[With reduction]{\label{code:loopnest_red}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_red.code}}}
~~~~~~~~~~
{\subfigcapskip = 5pt \subfigure[After pre-evaluation]{\label{code:loopnest_nored}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_nored.code}}}
\end{CenteredBox}\caption{Exposing (through factorization) and pre-evaluating a reduction.}\label{code:loopnest_rednored}\end{figure}

Pre-evaluation can be seen as the generalization of tensor contraction (Section~\ref{sec:tc}) to a wider class of sub-expressions. We know that multilinear forms can be seen as sums of monomials, each monomial being an integral over the equation domain of products (of derivatives) of functions from discrete spaces. A monomial can always be reduced to the product between a ``reference'' and a ``geometry'' tensors. In our model, a reference tensor is simply represented by one or more sub-expressions independent of $L_e$, exposed after particular transformations of the expression tree. This leads to the following algorithm. 

\begin{Algo}[Pre-evaluation]
\label{algo:pre-evaluation}
\normalfont
Consider a finite element integration loop nest $\Lambda = [L_e, L_i, L_j, L_k]$. We dissect the normal form input expression into distinct sub-expressions, each of them representing a monomial. Each sub-expression is then factorized so as to split constants from $[L_i, L_j, L_k]$-dependent terms. This transformation is feasible\footnote{For reasons of space, we omit the detailed sequence of steps (e.g., expansion, factorization), which is however available at \url{https://github.com/coneoproject/COFFEE/blob/master/coffee/optimizer.py}}, as a consequence of the results in~\citeN{Kirby:TC}. These $[L_i, L_j, L_k]$-dependent terms are hoisted outside of $\Lambda$ and stored into temporaries. As part of this process, the reduction induced by $L_i$ is computed by means of symbolic execution. Finally, $L_i$ is removed from $\Lambda$. 
\end{Algo}

The pre-evaluation of a monomial introduces some critical issues:
\begin{enumerate}
\item Depending on the complexity of a monomial, a certain number $t$ of temporary variables is required if pre-evaluation is performed. Such temporary variables are actually $n$-dimensional arrays of size $S$, with $n$ and $S$ being, respectively, the arity and the extent (iteration space size) of the multilinear loop nest (e.g., $n=2$ and $S = J K$ in the case of bilinear forms). For certain values of ${<}t, n, S{>}$, pre-evaluation will shift the loop nest from a CPU-bound to a memory-bound regime, which may be counter-productive for actual execution time.
\item The transformations exposing $[L_i, L_j, L_k]$-dependent terms increase the arithmetic complexity of the expression (e.g., expansion tends to increase the operation count). This could outweigh the gain due to pre-evaluation.
\item A strategy for coordinating sharing elimination and pre-evaluation is needed. We observe that sharing elimination inhibits pre-evaluation, whereas pre-evaluation could expose further sharing elimination opportunities.
\end{enumerate}

We expand on point 1) in the next section, while we address points 2) and 3) in Section~\ref{sec:optimal-synthesis}. 

\subsection{Memory constraints}
\label{sec:mem-const}
We have just observed that the code motion induced by monomial pre-evaluation may excessively increase the working set size. Interestingly, even more aggressive code motion strategies are theoretically conceivable. Imagine $\Lambda$ is enclosed in a time stepping loop. One could think of exposing (through some transformations) and hoisting time-invariant sub-expressions for minimizing redundant computation at each time step. The working set size would then increase by a factor $E$, and since $E \gg I, J, K$, the gain in operation count would probably be outweighed, from a runtime viewpoint, by a much larger memory pressure.

Since, for certain forms and discretizations, hoisting may cause the working set to exceed the size of some level of local memory (e.g. the last level of private cache on a conventional CPU, the shared memory on a GPU), we introduce the following \textit{memory constraints}.

\begin{Const}
\label{const:Le}
The size of a temporary due to code motion must not be proportional to the size of $L_e$.
\end{Const}

\begin{Const}
\label{const:TH}
The total amount of memory occupied by the temporaries due to code motion must not exceed a certain threshold, \texttt{$T_H$}.
\end{Const}

Constraint~\ref{const:Le} is a policy decision that the compiler should not silently consume memory on global data objects. It has the effect of shrinking the transformation space. Constraint~\ref{const:TH} has both theoretical and practical implications, which will be carefully analyzed in the next sections.

\subsection{Limitation in the optimization space considered}
\label{sec:completeness}
We defined sharing elimination and pre-evaluation as high level transformations on top of basic operators such as code motion and factorization. Factorization addresses \textit{spatial redundancy}. The presence of spatial redundancy means that some operations are needlessly executed at two points in an expression. Code motion and reduction pre-evaluation, on the other hand, target \textit{temporal redundancy}; that is, the needless execution of the same operation with the same operands at two points in the loop nest. We remark that sharing elimination and pre-evaluation aim to minimize spatial and temporal redundancy relying on \textit{a specific set of operators}. Theoretically, lower operation counts may be found by exploiting other domain-specific properties, such as redundancies in basis functions. Constraint~\ref{const:Le} also limits the applicability of code motion, which further reduced the transformation space to be analyzed. 

\section{Selection and composition of transformations}
\label{sec:optimal-synthesis}
In this section, we build a transformation algorithm that, given a memory bound, systematically produces quasi-optimal finite element integration loop nests. 

\subsection{Transformation algorithm}
We address the two following issues: 
\begin{enumerate}
\item \textit{Coordination of pre-evaluation and sharing elimination.} Recall from Section~\ref{sec:pre-evaluation} that pre-evaluation could either increase or decrease the operation count with respect to sharing elimination.
\item \textit{Optimizing over composite operations.} Consider a form comprising two monomials $m_1$ and $m_2$. Assume that pre-evaluation is profitable for $m_1$ but not for $m_2$, and that $m_1$ and $m_2$ share at least one term (e.g. some basis functions). If pre-evaluation were applied to $m_1$, sharing between $m_1$ and $m_2$ would be lost. We then need a mechanism to understand what transformation -- pre-evaluation or sharing elimination -- results in the highest operation count reduction when considering the whole set of monomials (i.e., the expression as a whole).
\end{enumerate}

Let $\theta : M \rightarrow \mathbb{Z}$ be a cost function that, given a monomial $m \in M$, returns the gain/loss achieved by pre-evaluation over sharing elimination. In particular, we define $\theta(m) = \mathrm{\theta}^{pre}(m) - \mathrm{\theta}^{se}(m)$, where $\mathrm{\theta}^{se}$ and $\mathrm{\theta}^{pre}$ represent the operation counts resulting from applying sharing elimination and pre-evaluation, respectively. Thus pre-evaluation is profitable for $m$ if and only if $\theta(m) < 0$. We return to the issue of deriving $\mathrm{\theta}^{se}$ and $\mathrm{\theta}^{pre}$ in Section~\ref{sec:op_count}. Having defined $\theta$, we can now describe the transformation algorithm (Algorithm~\ref{algo:gamma}).

\begin{Algo}[Transformation algorithm]
\label{algo:gamma}
\normalfont
The algorithm has three main phases: initialization (step 1); determination of the monomials preserving the memory constraints that should be pre-evaluated (steps 2-4); application of pre-evaluation and sharing elimination (step 5).
\begin{enumerate}
\item Perform a depth-first visit of the expression tree and determine the set of monomials $M$. Let $S$ be the subset of monomials $m$ such that $\theta(m) > 0$. The set of monomials that will \textit{potentially} be pre-evaluated is $P = M \setminus S$. \\ \textit{Note: there are two fundamental reasons for not pre-evaluating $m_1 \in P$ straight away: 1) the potential presence of spatial sharing between $m_1$ and $m_2 \in S$, which impacts the search for the global optimum; 2) the risk of breaking Constraint~\ref{const:TH}.}
\item Build the set $B$ of all possible bipartitions of $P$. Let $D$ be the dictionary that will store the operation counts of different alternatives.
\item Discard $b = (b_S, b_P) \in B$ if the memory required after applying pre-evaluation to the monomials in $b_P$ exceeds $T_H$ (see Constraint~\ref{const:TH}); otherwise, add $D[b] = \mathrm{\theta}^{se}(S \cup b_S) + \mathrm{\theta}^{pre}(b_P)$. \\ \textit{Note: $\mathbb{B}$ is in practice very small, since even complex forms usually have only a few monomials. This pass can then be accomplished rapidly as long as the cost of calculating $\mathrm{\theta}^{se}$ and $\mathrm{\theta}^{pre}$ is negligible. We elaborate on this aspect in Section~\ref{sec:op_count}.}
\item Take $\argmin_b D[b]$.
\item Apply pre-evaluation to all monomials in $b_P$. Apply sharing elimination to all resulting expressions. \\ \textit{Note: because of the reuse of basis functions, pre-evaluation may produce some identical tables, which will be mapped to the same temporary variable. Sharing elimination is therefore transparently applied to all expressions, including those resulting from pre-evaluation.}
\end{enumerate}
\end{Algo}

The output of the transformation algorithm is provided in Figure~\ref{code:loopnest-opt}, assuming as input the loop nest in Figure~\ref{code:loopnest}. 

\begin{figure}[h]\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_opt.code}
\end{CenteredBox}\caption{The loop nest produced by the algorithm for an input as in Figure~\ref{code:loopnest}.}\label{code:loopnest-opt}\end{figure}


\subsection{The cost function $\theta$}
\label{sec:op_count}
We tie up the remaining loose end: the construction of the cost function $\theta$.

We recall that $\theta(m) = \mathrm{\theta}^{se}(m) - \mathrm{\theta}^{pre}(m)$, with $\mathrm{\theta}^{se}$ and $\mathrm{\theta}^{pre}$ representing the operation counts after applying sharing elimination and pre-evaluation. Since $\theta$ is expected to be used by a compiler, requirements are simplicity and velocity. In the following, we explain how to derive these two values.

The most trivial way of evaluating $\mathrm{\theta}^{se}$ and $\mathrm{\theta}^{pre}$ would consist of applying the actual transformations and simply count the number of operations. This would be tolerable for $\mathrm{\theta}^{se}$, as Algorithm~\ref{algo:sharing-elimination} tends to have negligible cost. However, the overhead would be unacceptable if we applied pre-evaluation -- in particular, symbolic execution -- to all bipartitions analyzed by Algorithm~\ref{algo:gamma}. We therefore seek an analytic way of determining $\mathrm{\theta}^{pre}$.

The first step consists of estimating the \textit{increase factor}, $\iota$. This number captures the increase in arithmetic complexity due to the transformations exposing pre-evaluation opportunities. To contextualize, consider the example in Figure~\ref{code:increase_factor}. One can think of this as the (simplified) loop nest originating from the integration of a pre-multiplied mass matrix. The sub-expression \texttt{$f_0$*$b_{i0}$+$f_1$*$b_{i1}$+$f_2$*$b_{i2}$} represents the coefficient $f$ over (tabulated) basis functions (array $B$). In order to apply pre-evaluation, the expression needs be transformed to separate $f$ from all $[L_i, L_j, L_k]$-dependent quantities (see Algorithm~\ref{algo:pre-evaluation}). By product expansion, we observe an increase in the number of $[L_j, L_k]$-dependent terms of a factor $\iota = 3$.

\begin{figure}[h]\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_inc_factor.code}
\end{CenteredBox}\caption{Simplified loop nest for a pre-multiplied mass matrix.}\label{code:increase_factor}\end{figure}

In general, however, determining $\iota$ is not so straightforward since redundant tabulations may result from common sub-expressions. Consider the previous example. One may add one coefficient in the same function space as $f$, repeat the expansion, and observe that multiple sub-expressions (e.g., $b_{10}*b_{01}*...$ and $b_{01}*b_{10}*...$) will reduce to identical tables. To evaluate $\iota$, we then use combinatorics. We calculate the $k$-combinations with repetitions of $n$ elements, where: (i) $k$ is the number of (derivatives of) coefficients appearing in a product; (ii) $n$ is the number of unique basis functions involved in the expansion. In the original example, we had $n=3$ (for $b_{i0}$, $b_{i1}$, and $b_{i2}$) and $k=1$, which confirms $\iota=3$. In the modified example, there are two coefficients, so $k=2$, which means $\iota=6$.

If $\iota \geq I$ (the extent of the reduction loop), we already know that pre-evaluation will not be profitable. Intuitively, this means that we are introducing more operations than we are saving from pre-evaluating $L_i$. If $\iota < I$, we still need to find the number of terms $\rho$ such that $\mathrm{\theta}^{pre} = \rho \cdot \iota$. The mass matrix monomial in Figure~\ref{code:increase_factor} is characterized by the dot product of test and trial functions, so trivially $\rho = 1$. In the example in Figure~\ref{code:poisson}, instead, we have $\rho = 3$ after a suitable factorization of basis functions. In general, therefore, $\rho$ depends on both form and discretization. To determine this parameter, we look at the re-factorized expression (as established by Algorithm~\ref{algo:pre-evaluation}), and simply count the terms amenable to pre-evaluation.

\section{Formalization}
\label{sec:proof}
We demonstrate that the orchestration of sharing elimination and pre-evaluation as performed by the transformation algorithm guarantees quasi-optimality (Definition~\ref{def:mln-quasi-optimality}). The proof re-uses concepts and explanations provided throughout the paper, as well as the terminology introduced in Section~\ref{sec:se-algo}.

\begin{Prop}
\label{prop:optimal-approach}
Consider a multilinear form comprising a set of monomials $M$, and let $\Lambda$ be the corresponding finite element integration loop nest. Let $\Gamma$ be the transformation algorithm. Let $X$ be the set of monomials that, according to $\Gamma$, need be pre-evaluated, and let $Y = M \setminus X$. Assume that the pre-evaluation of different monomials does not result in identical tables. Then, $\Lambda' = \Gamma(\Lambda)$ is quasi-optimal in the sense of Definition~\ref{def:mln-quasi-optimality} and satisfies Constraint~\ref{const:TH}.
\end{Prop}
\begin{proof}
We first observe that the cost function $\theta$ predicts the \textit{exact} gain/loss in monomial pre-evaluation, so $X$ and $Y$ can actually be constructed.

Let $c_\Lambda$ denote the operation count for $\Lambda$ and let $\Lambda_I \subset \Lambda$ be the subset of innermost loops (all $L_k$ loops in Figure~\ref{code:loopnest-opt}). We need to show that there is no other synthesis $\Lambda''_{I}$ satisfying Constraint~\ref{const:TH} such that $c_{\Lambda''_I} < c_{\Lambda'_I}$. This holds if and only if
\begin{enumerate}
\item \textit{The coordination of pre-evaluation with sharing elimination is optimal}. This boils down to prove that
\begin{enumerate}
\item \textit{pre-evaluating any $m \in Y$ would result in $c_{\Lambda''_I} > c_{\Lambda'_I}$}
\item \textit{not pre-evaluating any $m \in X$ would result in $c_{\Lambda''_I} > c_{\Lambda'_I}$}
\end{enumerate}
\item \textit{Sharing elimination produces quasi-optimal (or optimal) loop nests.}\\
\end{enumerate}

We discuss these points separately

\begin{enumerate}
\item 
\begin{enumerate}
\item Let $T_m$ represent the set of tables resulting from applying pre-evaluation to a monomial $m$. Consider two monomials $m_1, m_2 \in Y$ and the respective sets of pre-evaluated tables, $T_{m_{1}}$ and $T_{m_{2}}$. If $T_{m_{1}} \cap\ T_{m_{2}} \neq \emptyset$, at least one table is assignable to the same temporary. $\Gamma$, therefore, may not be optimal, since $\theta$ only distinguishes monomials in ``isolation''. We neglect this scenario (see assumptions) because of its purely pathological nature and its -- with high probability -- negligible impact on the operation count.
\item Let $m_1 \in X$ and $m_2 \in Y$ be two monomials sharing some generic multilinear symbols. If $m_1$ were carelessly pre-evaluated, there may be a potential gain in sharing elimination that is lost, potentially leading to a non-optimum. This situation is prevented by construction, because $\Gamma$ exhaustively searches all possible bipartitions for determining an optimum preserving Constraint~\ref{const:TH}\footnote{Note that the problem can be seen as an instance of the well-known Knapsack problem}. Recall that since the number of monomials is in practice very small, this pass can rapidly be accomplished.
\end{enumerate}
\item Consider Algorithm~\ref{algo:sharing-elimination}. Proposition~\ref{prop:multi-struct} ensures that there are only two ways of scheduling the multilinear operands in $P \in \mathbb{P}$: through generalized code motion (Strategy~\ref{strategy:i}) or factorization of multilinear symbols (via Strategy~\ref{strategy:ii}). If applied, these two strategies would lead, respectively, to performing $|P|$ and $|S_P|$ multiplications at every loop iteration. Since Strategy~\ref{strategy:i} is applied if and only if $|P| < |S_P|$ and does not change the structure of the expression (it requires neither expansion nor factorization), step (3) cannot prune the optimum from the search space. 

Step (6) applies factorization following the ordering dictated by the sharing graph and its rewrite rules. We here demonstrate why this is a step forwards towards the optimum. Note that, at this point, non flop-decreasing factorizations have already been pruned (step (5)). 

Instances of rules (A) and (B) represent ``unavoidable'' factorizations, for which one multiplication must be paid regardless of how the applications of Strategy~\ref{strategy:ii} are scheduled. In particular:
\begin{description}
\item[Rule (A)] As terminals, $\lbrace T^0, ..., T^n \rbrace$ can only be grouped through the factorization of $S$, so this rule has no side effects.
\item[Rule (B)] Since $S^{nt}$ is not a terminal, there must exist at least one edge between $S^{nt}$ and a generic vertex $V$. Let us consider the alternative factorization $S^{nt}(...S + ... + ...V)$. If $V$ were a terminal, we see that setting $V = T$ and $S = S^{nt}$ would simply expose another hit for rule (B). Otherwise ($V$ non terminal), we would have $S$ now appearing in two different sub-expressions ($(...S + ... + ...V)$ and $(...S\ T)$), which would clearly be sub-optimal as a consequence of the spatial sharing induced by $S$. From this, we evince that applying rules (C) and (D) prior to rule (B) may prune the optimum from the search space.
\end{description}

Rule (C) is the critical one. Let $\mathbb{A}$ be an acyclic sub-graph rooted in $S$, and let $W$ be the neighbor of $S$ in $\mathbb{A}$. Since $\mathbb{T} = \emptyset$, $S$ is also the root a second sub-graph $\mathbb{C}$. The non-trivial case in when $\mathbb{C}$ is cyclic. If any vertex in $\mathbb{C}$ were factorized prior to $S$, the product $S\ W$ could eventually appear in the transformed expression. Conversely, is Strategy~\ref{strategy:ii} were first applied to $S$, $W$ and one or more symbols in $\mathbb{C}$ would be grouped in the same sub-expression and hoisted. 
 
Finally, note that the algorithm is guaranteed to terminate because of the jump to step (7) as $\mathbb{S} = \mathbb{T}$. 
\end{enumerate}
\end{proof}


Throughout the paper we have reiterated the idea that Algorithm~\ref{algo:gamma} reaches optimality if stronger preconditions on the input variational form are satisfied. We here discuss such preconditions, in increasing order of complexity.
\begin{enumerate}
\item There is a single monomial and only a specific coefficient (e.g., the coordinates field). This is by far the simplest scenario, which requires no particular transformation at the level of the outer loops, so optimality naturally follows.
\item There is a single monomial, but multiple coefficients are present. Optimality is achieved if and only if all sub-expressions depending on coefficients are structured (see Section~\ref{sec:se-rln}). This would avoid ambiguity in factorization, which in turn would guarantee the output of step (7) in Algorithm~\ref{algo:sharing-elimination} be optimality.
\item There are multiple monomials, but either at most one coefficient (e.g., the coordinates field) or multiple coefficients not inducing sharing across different monomials are present. This reduces, respectively, to cases (1) and (2) above.
\item There are multiple monomials, and coefficients are shared across monomials. Optimality is reached if and only if the coefficient-dependent sub-expressions produced by Algorithm~\ref{algo:sharing-elimination} -- that is, the by-product of factorizing test/trial functions from distinct monomials -- preserve structure. 
\end{enumerate}


\section{Code Generation}
\label{sec:codegen}
Sharing elimination and pre-evaluation, as well as the transformation algorithm, have been implemented in COFFEE, the compiler for finite element integration routines adopted in Firedrake. In this section, we briefly discuss the aspects of the compiler that are relevant for this article.

\subsection{Expressing transformations through the COFFEE language}
COFFEE implements sharing elimination and pre-evaluation by composing ``building-block'' operators, which we refer to as ``rewrite operators''. This has several advantages. Firstly, extendibility: novel transformations -- for instance, sum-factorization in spectral methods -- could be expressed using the existing operators, or with small effort building on what is already available. Secondly, generality: COFFEE can be seen as a lightweight, low level computer algebra system, not necessarily tied to finite element integration. Thirdly, robustness: the same operators are exploited, and therefore stressed, by different optimization pipelines. The rewrite operators, whose (Python) implementation is based on manipulation of abstract syntax trees (ASTs), compose the COFFEE language. A non-exhaustive list of such operators includes expansion, factorization, re-association, generalized code motion.

\subsection{Independence from form compilers}
COFFEE aims to be independent of the high level form compiler. It provides an interface to build generic ASTs and only expects expressions to be in normal form (or sufficiently close to it). In Firedrake, for example, a modified version of the FEniCS Form Compiler producing ASTs, instead of strings, is used. Thus, COFFEE decouples the mathematical manipulation of a form from code optimization; or, in other words, relieves form compiler developers of caring about performance of generated code.

\subsection{Handling block-sparse tables}
\label{sec:zeros}
For several reasons, basis function tables may be block-sparse (e.g., containing zero-valued columns). For example, the FEniCS Form Compiler implements vector-valued functions by adding blocks of zero-valued columns to the corresponding tabulations; this extremely simplifies code generation (particularly, the construction of loop nests), but also affects the performance of the generated code due to the execution of ``useless'' flops (e.g., operations like \texttt{a + 0}). In~\citeN{quadrature1}, a technique to avoid iteration over zero-valued columns based on the use of indirection arrays (e.g. \texttt{A[B[i]]}, in which \texttt{A} is a tabulated basis function and \texttt{B} a map from loop iterations to non-zero columns in A) was proposed. This technique, however, promotes non-contiguous memory loads and stores, which nullify the potential benefits of vectorization. COFFEE, instead, handles block-sparse basis function tables by restructuring loops in a way that low level optimization (especially vectorization) is only marginally affected. This is based on symbolic execution of the code, which enables a series of checks on array indices and loop bounds for determining which zero-valued blocks can be skipped without affecting data alignment.

\section{Performance Evaluation}
\label{sec:perf-results}

\subsection{Experimental setup}

Experiments were run on a single core of an Intel I7-2600 (Sandy Bridge) CPU, running at 3.4GHz, 32KB L1 cache (private), 256KB L2 cache (private) and 8MB L3 cache (shared). The Intel Turbo Boost and Intel Speed Step technologies were disabled. The Intel \texttt{icc 15.2} compiler was used. The compilation flags used were \texttt{-O3, -xHost}.

We analyze the execution time of four real-world bilinear forms of increasing complexity, which comprise the differential operators that are most common in finite element methods. In particular, we study the mass matrix (``\texttt{Mass}'') and the bilinear forms arising in a Helmholtz equation (``\texttt{Helmholtz}''), in an elastic model (``\texttt{Elasticity}''), and in a hyperelastic model (``\texttt{Hyperelasticity}''). The complete specification of these forms is made publicly available\footnote{\url{https://github.com/firedrakeproject/firedrake-bench/blob/experiments/forms/firedrake_forms.py}}. 

We evaluate the speed-ups achieved by a wide variety of transformation systems over the ``original'' code produced by the FEniCS Form Compiler (i.e., no optimizations applied). We analyze the following transformation systems:
\begin{description}
\item[quad] Optimized quadrature mode. Work presented in~\citeN{quadrature1}, implemented in  in the FEniCS Form Compiler. 
\item[tens] Tensor contraction mode. Work presented in~\citeN{FFC-TC}, implemented in the FEniCS Form Compiler.
\item[auto] Automatic choice between \texttt{tens} and \texttt{quad} driven by heuristic (detailed in~\citeN{Fenics} and summarized in Section~\ref{sec:qualitative}). Implemented in the FEniCS Form Compiler.
\item[ufls] UFLACS, a novel back-end for the FEniCS Form Compiler whose primary goals are improved code generation and execution times.
\item[cfO1] Generalized loop-invariant code motion. Work presented in~\citeN{Luporini}, implemented in COFFEE.
\item[cfO2] Optimal loop nest synthesis with handling of block-sparse tables. Work presented in this article, implemented in COFFEE.
\end{description}

The values that we report are the average of three runs with ``warm cache'' (no code generation time, no compilation time). They include the cost of both local assembly and matrix insertion, with the latter minimized through the choice of a mesh (details below) small enough to fit the L3 cache of the CPU. 

For a fair comparison, small patches (publicly available) were written for the compatibility of \texttt{quad}, \texttt{tens}, and \texttt{ufls} with Firedrake. By executing all simulations in Firedrake, we guarantee that both matrix insertion and mesh iteration have a fixed cost, independent of the transformation system employed. The patches adjust the data storage layout to what Firedrake expects (e.g., by generating an array of pointers instead of a pointer to pointers, by replacing flattened arrays with bi-dimensional ones).

For Constraint~\ref{const:TH}, discussed in Section~\ref{sec:mem-const}, we set $T_H = L2_{size}$; that is, the size of the processor L2 cache (the last level of private cache). When the threshold had an impact on the transformation process, the experiments were repeated with $T_H = L3_{size}$. The results are documented later, individually for each problem.

Following the methodology adopted in~\citeN{quadrature1}, we vary the following parameters:
\begin{itemize}
\item the polynomial order of test, trial, and coefficient (or ``pre-multiplying'') functions, $q \in \lbrace1, 2, 3, 4\rbrace$
\item the number of coefficient functions $nf \in \lbrace0, 1, 2, 3\rbrace$
\end{itemize}
While constants of our study are
\begin{itemize}
\item the space of test, trial, and coefficient functions: Lagrange
\item the mesh: tetrahedral with a total of 4374 elements
\item exact numerical quadrature (we employ the same scheme used in~\citeN{quadrature1}, based on the Gauss-Legendre-Jacobi rule)
\end{itemize}

\subsection{Performance results}
\label{sec:perf-results-forms}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/mass}}
\caption{Performance evaluation for the \textit{mass} matrix. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:mass}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/helmholtz}}
\caption{Performance evaluation for the bilinear form of a \textit{Helmholtz} equation. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:helmholtz}
\end{figure}

\begin{figure}
\includegraphics[scale=0.77]{perf-results/elasticity}
\caption{Performance evaluation for the bilinear form arising in an \textit{elastic} model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:elasticity}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/hyperelasticity}}
\caption{Performance evaluation for the bilinear form arising in a \textit{hyperelastic} model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:hyperelasticity}
\end{figure}

We report the results of our experiments in Figures~\ref{fig:mass},~\ref{fig:helmholtz},~\ref{fig:elasticity}, and~\ref{fig:hyperelasticity} as three-dimensional plots. The axes represent $q$, $nf$, and code transformation system. We show one subplot for each problem instance $\langle form, nf, q\rangle$, with the code transformation system varying within each subplot. The best variant for each problem instance is given by the tallest bar, which indicates the maximum speed-up over non-transformed code. We note that if a bar or a subplot are missing, then the form compiler failed at generating code because of either exceeding the system memory limit or unable to handle the form. 

The rest of the section is organized as follows: we first provide insights into the overall outcome of the experimentation; we then comment on the impact of a fundamental low-level optimization, namely autovectorization; finally, we explain, in detail and individually for each form, the performance results obtained.

\paragraph{High level view}
The main observation is that our transformation strategy does not always guarantee minimum execution time. In particular, 5$\%$ of the test cases (3 out of 56, without counting marginal differences) show that \texttt{cfO2} was not optimal in terms of runtime. The most significant of such test cases is the elastic model with $[q=4, nf=0]$. There are two reasons for this. Firstly, low level optimization can have a significant impact on actual performance. For example, the aggressive loop unrolling in \texttt{tens} eliminates operations on zeros and reduces the working set size by not storing entire temporaries; on the other hand, preserving the loop structure can maximize the chances of autovectorization. Secondly, memory constraints are critical, particularly the transformation strategy adopted when exceeding $T_H$. We will later thoroughly elaborate on all these aspects.

\paragraph{Autovectorization}
The discretizations employed result in inner loops and basis function tables of size multiple of the machine vector length. This, combined with the chosen compilation flags, promotes autovectorization in the majority of code variants. An exception is \texttt{quad} due to the presence of indirection arrays in the generated code. In \texttt{tens}, loop nests are fully unrolled, so the standard loop vectorization is not feasible; manual inspection of the compiled code suggests, however, that block vectorization (\cite{SLP-vect}) is often triggered. In \texttt{ufls}, \texttt{cfO1}, and \texttt{cfO2} the iteration spaces have similar structure (there are a few exceptions in \texttt{cfO2} due to zero-elimination), with loop vectorization being regularly applied, as far as we could evince from compiler reports and manual inspection of assembly code.

\paragraph{Mass}
We start with the simplest of the bilinear forms investigated, the mass matrix. Results are in Figure~\ref{fig:mass}. We first notice that the lack of improvements when $q=1$ is due to the fact that matrix insertion outweighs local assembly. As $q \geq 2$, \texttt{cfO2} generally shows the highest speed-ups. It is worth noting how \texttt{auto} does not always select the fastest implementation: \texttt{auto} always opts for \texttt{tens}, while as $nf \geq 2$ \texttt{quad} would tend to be preferable. On the other hand, \texttt{cfO2} always makes the optimal decision about whether applying pre-evaluation or not.  

\paragraph{Helmholtz}
As happened with the mass matrix problem, when $q=1$ matrix insertion still hides the cost of local assembly. For $q \geq 2$, the general trend is that \texttt{cfO2} outperforms the competitors. In particular, with
\begin{itemize}
\item $nf=0$, the adoption of pre-evaluation by \texttt{cfO2} results in increasingly notable speed-ups over \texttt{cfO1}, as $q$ increases; \texttt{tens} is comparable to \texttt{cfO2}, with \texttt{auto} making the right choice. 
\item $nf=1$, \texttt{auto} picks \texttt{tens}; the choice is however sub-optimal when $q=3$ and $q=4$. This can indirectly be inferred from the large gap between \texttt{cfO1/cfO2} and \texttt{tens/auto}: \texttt{cfO2} applies sharing elimination, but it avoids pre-evaluation. 
\item $nf=2$ and $nf=3$, \texttt{auto} reverts to \texttt{quad}, which would theoretically be the right choice (the flop count is much lower than in \texttt{tens} or what would be produced by pre-evaluation); however, the generated code suffers from the presence of indirection arrays, which break autovectorization and ``traditional'' code motion.
\end{itemize}

The sporadic slow-downs or only marginal improvements exhibited by \texttt{ufls} are imputable to the presence of sharing.

An interesting experiment we performed was relaxing the memory threshold by setting it to $T_H = L3_{size}$. We found that this makes \texttt{cfO2} generally slower as $nf \geq 2$, with a maximum slow-down of 2.16$\times$ with $\langle nf=2, q=2\rangle$. The effects of not having a sensible threshold could even be worse in parallel runs, since the L3 cache is shared by the cores.

\paragraph{Elasticity}
The results for the elastic model are displayed in Figure~\ref{fig:elasticity}. The main observation is that \texttt{cfO2} never triggers pre-evaluation, although in some occasions it should. To clarify this, consider the test case $\langle nf=0, q=2 \rangle$, in which \texttt{tens/auto} show a considerable speed-up over \texttt{cfO2}. \texttt{cfO2} finds pre-evaluation profitable -- that is, actually capable of reducing the operation count -- although it does not apply it because otherwise $T_H$ would be exceeded. However, running the same experiments with $T_H = L3_{size}$ resulted in a dramatic improvement, even higher than that of \texttt{tens}. Our explanation is that despite exceeding $T_H$ by roughly 40$\%$, the save in operation count is so large (5$\times$ in this problem) that pre-evaluation would anyway be the optimal choice. This suggests that our model could be refined to handle the cases in which there is a significant gap between potential cache misses and save in flops.

We also note that:
\begin{itemize}
\item the differences between \texttt{cfO2} and \texttt{cfO1} are due to systematic sharing elimination and the use of symbolic execution to avoid iteration over the zero-valued regions in the basis function tables
\item when $nf=1$, \texttt{auto} prefers \texttt{tens} to \texttt{quad}, which leads to sub-optimal operation counts and execution times
\item \texttt{ufls} generally shows better runtime behaviour than \texttt{quad} and \texttt{tens}. This is due to multiple facts, including avoidance of indirection arrays, preservation of loop structure, a more effective code motion.
\end{itemize}

\paragraph{Hyperelasticity}
In the experiments on the hyperelastic model, shown in Figure~\ref{fig:hyperelasticity}, \texttt{cfO2} exhibits the largest gains out of all problem instances considered in this paper. This is a positive aspect, since it indicates that our transformation algorithm scales well with form complexity. The fact that all code transformation systems (apart from \texttt{tens}) show quite significant speed-ups suggests two points. Firstly, the baseline is highly inefficient. With forms as complex as in the hyperelastic model, a trivial translation of integration routines into code should always be avoided as even the best general-purpose compiler available (Intel's on an Intel platform at maximum optimization level) cannot exploit the structure inherent in the expressions. Secondly, the strategy for removing spatial and temporal sharing has a tremendous impact. Sharing elimination as performed by \texttt{cfO2} ensures a critical reduction in operation count, which becomes particularly pronounced for higher values of $q$. 


\section{Conclusions}
\label{sec:conclusions}
We have developed transformation algorithms and a theory of optimality for finite element integration. The implementation was carried out in COFFEE, a compiler fully integrated with the Firedrake framework, and therefore made available to an increasingly significant user base. Performance results show the effectiveness of our methodology. There are still several open problems. The main one is an elaborate classification of forms for which optimality can be reached. Secondly, we would like to extend our methodology to classes of loops arising in spectral methods, in which interaction with low level optimization becomes even more critical due to the larger working sets.

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{biblio}


\medskip

\end{document}
