% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{tabulary}
\usepackage{fancybox}

% font in listings
\usepackage{courier}

\makeatletter
\newenvironment{CenteredBox}{% 
\begin{Sbox}}{% Save the content in a box
\end{Sbox}\centerline{\parbox{\wd\@Sbox}{\TheSbox}}}% And output it centered
\makeatother

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{lineno}
\usepackage{xfrac}

% Theorems, definitions, lemmas, etc
\newtheorem{Def}{Definition}
\newtheorem{Prop}{Proposition}
\newtheorem*{Rem*}{Remark}

\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}

\usepackage{listings}
\lstset{language=C, breaklines=true, mathescape}

\usepackage[cmex10]{amsmath}
\usepackage{url}



% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}

% Document starts
\begin{document}

% Page heads
\markboth{F. Luporini et al.}{On Optimality of Finite Element Integration}

% Title portion
\title{On Optimality of Finite Element Integration}
\author{Fabio Luporini
\affil{Imperial College London}
David A. Ham
\affil{Imperial College London}
Paul H.J. Kelly
\affil{Imperial College London}}

\begin{abstract}
We tackle the problem of automatically generating optimal finite element integration routines given a high level specification of arbitrary multilinear forms. Optimality is defined in terms of floating point operations required to execute a loop nest. The generation of optimal loop nests is driven by a model that exploits mathematical properties of the domain of interest. A theoretical analysis and extensive experimentation prove the effectiveness of our approach, showing systematic performance improvements over a number of alternative code generation systems. The effect of low-level optimization is also discussed.
\end{abstract}

\category{G.1.8}{Numerical Analysis}{Partial Differential Equations -
  Finite element methods}

\category{G.4}{Mathematical Software}{Parallel and vector implementations}

\terms{Design, Performance}

\keywords{Finite element integration, local assembly, compilers, performance optimization}

\acmformat{Fabio Luporini, David A. Ham, and Paul   H. J. Kelly, 2015. On Optimality of Finite Element Integration.}

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT: Full first names whenever they are known, surname last,
% followed by a period.  In the case of two authors, 'and' is placed
% between them.  In the case of three or more authors, the serial
% comma is used, that is, all author names except the last one but
% including the penultimate author's name are followed by a comma, and
% then 'and' is placed before the final author's name.  If only first
% and middle initials are known, then each initial is followed by a
% period and they are separated by a space.  The remaining information
% (journal title, volume, article number, date, etc.) is
% 'auto-generated'.

\begin{bottomstuff}

This research is partly funded by the MAPDES project, by the
Department of Computing at Imperial College London, by EPSRC through
grants EP/I00677X/1, EP/I006761/1, and EP/L000407/1, by NERC grants
NE/K008951/1 and NE/K006789/1, by the U.S.  National Science
Foundation through grants 0811457 and 0926687, by the U.S. Army
through contract W911NF-10-1-000, and by a HiPEAC collaboration
grant. The authors would like to thank Mr. Andrew T.T. McRae,
Dr. Lawrence Mitchell, and Dr. Francis Russell for their invaluable
suggestions and their contribution to the Firedrake project.

Author's addresses: Fabio Luporini $\&$ Paul H. J. Kelly, Department of Computing,
Imperial College London; David A. Ham, Department of Computing and
Department of Mathematics, Imperial College London; 
\end{bottomstuff}

\maketitle


\section{Introduction}

The need for rapidly implementing high performance, robust, and portable finite element methods has led to approaches based on automated code generation. This has been proved successful in the context of the FEniCS (\cite{Fenics}) and Firedrake (\cite{firedrake-code}) projects, which have become increasingly popular over the last years. In these frameworks, the weak variational form of a problem is expressed at high-level by means of a domain-specific language. The mathematical specification is manipulated by a form compiler that generates a representation of assembly operators. By applying these operators to an element in the discretized domain, a local matrix and a local vector, which represent the contributions of that element to the equation solution, are computed. The code for assembly operators should be high performance: as the complexity of a variational form increases, in terms of number of derivatives, pre-multiplying functions, or polynomial order of the chosen function spaces, the operation count increases, with the result that assembly often accounts for a significant fraction of the overall runtime. 

As demonstrated by the considerable body of research on the topic, automating the generation of such high performance implementations poses several challenges. This is a result of the complexity inherent to the mathematical expressions involved in the numerical integration, which varies from problem to problem, and the particular structure of the loop nests enclosing the integrals. General-purpose compilers, such as \emph{GNU's} and \emph{Intel's}, fail at exploiting the structure inherent in the expressions, thus producing sub-optimal code (i.e., code which performs more floating-point operations, or ``flops'', than necessary). Research compilers, for instance those based on polyhedral analysis of loop nests such as PLUTO (\cite{PLUTO}), focus on parallelization and loop optimization for cache locality, so they are not particularly helpful in our context. The lack of suitable third-party tools has led to the development of a number of domain-specific code transformation (or synthesizer) systems. In~\citeN{quadrature1}, it is shown how automated code generation can be leveraged to introduce optimizations that a user should not be expected to write ``by hand''. In~\citeN{FFC-TC} and~\citeN{Francis}, mathematical reformulations of finite element integration are studied with the aim of minimizing the operation count. In~\citeN{Luporini}, the effects and the interplay of generalized code motion and a set of low-level optimizations are analysed. It is also worth mentioning an on-going effort to produce a novel form compiler, called UFLACS (\cite{Uflacs}), which adds to the already abundant set of code transformation systems for assembly operators. 

However, in spite of such a considerable research effort, still there is no answer to one fundamental question: can we automatically generate an implementation of a form which is optimal in the number of flops executed? In this paper, we formulate an approach to solve this problem. Summarizing, our contributions are as follows

\begin{itemize}
\item We characterize flop-optimality in a loop nest and we instantiate this concept to finite element integration. As part of this construction, we establish the notion of sharing and demonstrate that sharing can always be eradicated from the loop nests we are interested in.
\item We provide a model centred on sharing and other mathematical properties of our domain of interest; the model drives the translation of a monomial appearing in a form into flop-optimal loop nests.
\item We comment on the cases in which such model does not lead to optimal loop nests. We will see that this might occur with particular forms that make extensive use of tensor algebra. We show, however, that our model still is capable of producing quasi-optimal loop nests.
\item We integrate the model with a compiler, COFFEE\footnote{COFFEE stands for COmpiler For Fast Expression Evaluation. The compiler is open-source and available at \url{https://github.com/coneoproject/COFFEE}}, which is in use in the Firedrake framework.
\item We experimentally evaluate using a broader suite of forms, discretizations, and code generation systems than has been used in prior research. This is essential to demonstrate that our optimality model holds in practice.
\end{itemize}

In addition, in order to place COFFEE on the same level of other code generation systems from the viewpoint of low-level optimization (which is essential for a fair performance comparison)

\begin{itemize}
\item We introduce an engine based on symbolic execution that allows skipping irrelevant floating point operations (e.g., those involving zero-valued quantities). We elaborate on the performance impact of this optimization, making a clear distinction between flop optimality and efficient code.
\end{itemize}


\section{Preliminaries}
\label{sec:background}
We review finite element integration using the same notation and examples adopted in~\citeN{quadrature1} and~\citeN{Francis}. 

We consider the weak formulation of a linear variational problem
\begin{equation}
\begin{split}
Find\ u\ \in U\ such\ that \\
a(u, v) = L(v), \forall v \in V
\end{split}
\end{equation}
where $a$ and $L$ are, respectively, a bilinear and a linear form. The set of \textit{trial} functions $U$ and the set of \textit{test} functions $V$ are discrete function spaces. For simplicity, we assume $U = V$. Let $\lbrace \phi_i \rbrace$ be the set of basis functions spanning $U$. The unknown solution $u$ can be approximated as a linear combination of the basis functions $\lbrace \phi_i \rbrace$. From the solution of the following linear system it is possible to determine a set of coefficients to express $u$:
\begin{equation}
A\textbf{u} = b
\end{equation}
in which $A$ and $b$ discretize $a$ and $L$ respectively:
\begin{equation}
\centering
\begin{split}
A_{ij} = a(\phi_i(x), \phi_j(x)) \\
b_i = L(\phi_i(x))
\end{split}
\end{equation}
The matrix $A$ and the vector $b$ are ``assembled'' and subsequently used to solve the linear system through (typically) an iterative method.

We focus on the assembly phase, which is often characterized as a two-step procedure: \textit{local} and \textit{global} assembly. Local assembly is the subject of this article. It consists of computing the contributions of a single element in the discretized domain to the equation solution. In global assembly, such local contributions are ``coupled'' by suitably inserting them into $A$ and $b$. 

We illustrate local assembly in a concrete example, the evaluation of the local element matrix for a Laplacian operator. Consider the weighted Laplace equation
\begin{equation}
- \nabla \cdot (w \nabla u) = 0
\end{equation}
in which $u$ is unknown, while $w$ is prescribed. The bilinear form associated with the weak variational form of the equation is:
\begin{equation}
a(v, u) = \int_\Omega w \nabla v \cdot \nabla u\ \mathrm{d}x
\end{equation}
The domain $\Omega$ of the equation is partitioned into a set of cells (elements) $T$ such that $\bigcup T = \Omega$ and $\bigcap T = \emptyset$. By defining $\lbrace \phi_i^K \rbrace$ as the set of local basis functions spanning $U$ on the element $K$, we can express the local element matrix as
\begin{equation}
\label{stiffness}
A_{ij}^K = \int_K w \nabla \phi_i^K \cdot \nabla \phi_j^K\ \mathrm{d}x
\end{equation}
The local element vector $L$ can be determined in an analogous way. 
%From the computational perspective, its evaluation is however less expensive than that of $A$.

\subsection{Quadrature Mode}
Quadrature schemes are typically used to numerically evaluate $A_{ij}^K$. For convenience, a reference element $K_0$ and an affine mapping $F_K : K_0 \rightarrow K$ to any element $K \in T$ are introduced. This implies that a change of variables from reference coordinates $X_0$ to real coordinates $x = F_K (X_0)$ is necessary any time a new element is evaluated. The numerical integration routine based on quadrature over an element $K$ can be expressed as follows
\begin{equation}
\label{eq:quadrature}
A_{ij}^K = \sum_{q=1}^N \sum_{\alpha_3=1}^n \phi_{\alpha_3}(X^q)w_{\alpha_3} \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\beta=1}^d \frac{\partial X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial \phi_i^K(X^q)}{\partial X_{\alpha_1}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \frac{\partial \phi_j^K(X^q)}{\partial X_{\alpha_2}} det F_K' W^q
\end{equation}
where $N$ is the number of integration points, $W^q$ the quadrature weight at the integration point $X^q$, $d$ is the dimension of $\Omega$, $n$ the number of degrees of freedom associated to the local basis functions, and $det$ the determinant of the Jacobian matrix used for the aforementioned change of coordinates.  

% Magari la summation sui coefficients la sputo dentro? che dici?

\subsection{Tensor Contraction Mode}
Starting from Equation~\ref{eq:quadrature}, exploiting linearity, associativity and distributivity of the involved mathematical operators, we can rewrite the expression as
\begin{equation}
\label{eq:tensor}
A_{ij}^K = \sum_{\alpha_1=1}^d \sum_{\alpha_2=1}^d \sum_{\alpha_3=1}^n det F_K' w_{\alpha_3} \sum_{\beta=1}^d \frac{X_{\alpha_1}}{\partial x_{\beta}} \frac{\partial X_{\alpha_2}}{\partial x_{\beta}} \int_{K_0} \phi_{\alpha_3} \frac{\partial \phi_{i_1}}{\partial X_{\alpha_1}} \frac{\partial \phi_{i_2}}{\partial X_{\alpha_2}} dX.
\end{equation}
A generalization of this transformation has been proposed in~\cite{Kirby:TC}. By only involving reference element terms, the integral in the equation can be pre-evaluated and stored in temporary variables. The evaluation of the local tensor can then be abstracted as
\begin{equation}
A_{ij}^K = \sum_{\alpha} A_{i_1 i_2 \alpha}^0 G_{K}^\alpha
\end{equation}
in which the pre-evaluated ``reference tensor'' $A_{i_1 i_2 \alpha}$ and the cell-dependent ``geometry tensor'' $G_{K}^\alpha$ are exposed. 

\subsection{Qualitative Comparison}
\label{sec:qualitative}
Depending on form and discretization, the relative performance of the two modes, in terms of the operation count, can vary quite dramatically. The presence of derivatives or coefficient functions in the input form tends to increase the size of the geometry tensor, making the traditional quadrature mode preferable for ``complex'' forms. On the other hand, speed-ups from adopting tensor mode can be significant in a wide class of forms in which the geometry tensor remains ``sufficiently small''. The discretization, particularly the relative polynomial order of trial, test, and coefficient functions, also plays a key role in the resulting operation count. 

These two modes have been implemented in the FEniCS Form Compiler (\cite{FFC-TC}). In this compiler, a heuristic is used to choose the most suitable mode for a given form. It consists of analysing each monomial in the form, counting the number of derivatives and coefficient functions, and checking if this number is greater than a constant found empirically (\cite{Fenics}). We will later comment on the efficacy of this approach (Section~\ref{sec:optimal-synthesis}. For the moment, we just recall that one of the goals of this research is to produce an intelligent system that goes beyond the dichotomy between quadrature and tensor modes. We will reason in terms of loop nests, code motion, and code pre-evaluation, searching the entire implementation space for an optimal synthesis.  

\section{Problem Characterization}
\label{sec:optimal-impl}

In this section, we characterize optimality for finite element integration, as well as the transformation space that needs be explored to achieve it. 

\subsection{Loop Nests and Optimality}
In order to make the document self-contained, we start with reviewing basic compiler terminology.

\begin{Def}[Perfect and imperfect loop nests]
A perfect loop nest is a loop whose body either 1) comprises only a sequence
of non-loop statements or 2) is itself a perfect loop nest. If this
condition does not hold, a loop nest is said to be imperfect. 
\end{Def}

\begin{Def}[Independent basic block]
An independent basic block is a sequence of statements such that no data
dependencies exist between statements in the block.
\end{Def}

We focus on perfect nests whose innermost loop body is an independent basic
block. A straightforward property of this class is that hoisting invariant
expressions from the innermost to any of the outer loops or the preheader
(i.e., the block that precedes the entry point of the nest) is always safe,
as long as any dependencies on loop indices are honored. We will make use of this property. The results of this section could also be generalized to larger classes of loop nests, in which basic block independence does not hold, although this would require refinements beyond the scope of this paper. 

By mapping mathematical properties to the loop nest level, we introduce the
concepts of a \textit{linear loop} and, more generally, a (perfect) multilinear loop nest.

\begin{Def}[Linear loop]
A loop $L$ defining the iteration space $I$ through the iteration variable $i$, or simply $L_i$, is \textit{linear} if 
\begin{enumerate}
\item $i$ appears in the body of $L$ only as an array index, and
\item whenever an array $a$ is indexed by $i$ ($a[i]$), all expressions in which this appears are affine in $a$.
\end{enumerate}
\end{Def}

\begin{Def}[Perfect multilinear loop nest]
A perfect multilinear loop nest of arity $n$ is a perfect nest composed of $n$ loops, in which all of the expressions appearing in the body of the innermost loop are linear in each loop $L_i$ separately.
\end{Def}

Since our focus is on finite element integration, for simplicity we restrict ourselves to the following, notable subclass of perfect multilinear loop nests.

\begin{Def}[Outer-product loop nest]
An outer-product loop nest is a perfect multilinear loop nest of arity $n \leq 2$ in which all of the expressions appearing in the body of the innermost loop are summations of outer products.
\end{Def}

As shown later, outer-product loop nests are important because they possess properties that we can take advantage of to synthesize optimal code. They arise naturally when translating bilinear or linear forms into code. Consider Equation~\ref{eq:quadrature} and the (abstract) loop nest implementing it illustrated in Figure~\ref{code:loopnest}. The imperfect nest $\Lambda=[L_e, L_i, L_j, L_k]$ comprises the element loop $L_e$, the integration loop $L_i$ (a reduction loop), and the outer-product loop nest $[L_j, L_k]$ over test and trial functions. The expression \texttt{F} implements the operator.

% (the Laplacian operator in the case of Equation~\ref{eq:quadrature}, but our discussion applies in general).
\begin{figure}[h]\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest.code}
\end{CenteredBox}\caption{The typical loop nest implementing a bilinear form.}\label{code:loopnest}\end{figure}

Our aim is a strategy to synthesize optimal loop nests of this form. In particular, we characterize optimality as follows.

\begin{Def}[Optimality of a loop nest]
\label{def:mln-optimality}
Let $\Lambda$ be a generic loop nest, and let $\Gamma$ be a generic transformation function $\Gamma : \Lambda \rightarrow \Lambda'$ such that $\Lambda'$ is semantically equivalent to $\Lambda$ (possibly, $\Lambda' = \Lambda$). We say that $\Lambda' = \Gamma (\Lambda)$ is an optimal synthesis of $\Lambda$ if the number of operations that it performs to evaluate the result is minimal.
\end{Def}

Note that Definition~\ref{def:mln-optimality} does not take into account memory requirements. If the loop nest were memory-bound -- the ratio of operations to bytes transferred from memory to the CPU being too low -- then speaking of optimality would clearly make no sense. Henceforth we assume to operate in a CPU-bound regime, in which arithmetic-intensive expressions need be evaluated. In the context of finite element, this is often true for more complex multilinear forms and/or higher order elements.

\subsection{Transformation Space}
To synthesize optimal implementations we need:
\begin{itemize}
\item a characterization of the transformation space for the class of loop nests considered
\item a cost model to select the optimal point in the transformation space
\end{itemize}
In this section, we construct the transformation space. We leave a few loose hands, which we progressively tie in Section~\ref{sec:optimal-synthesis}.

We start with introducing the fundamental notion of sharing.

\begin{Def}[Sharing]
A loop $L_i$ presents sharing if it contains at least one statement $S$ in which two (sub-)expressions depending on $i$ are symbolically identical.
\end{Def}

\begin{Def}[Sharing factor]
The sharing factor of a loop $L_i$, $L^{sf}_i$, is given by the cardinality of the set of symbols depending on $i$ (i.e., arrays indexed by $i$). It provides an intuition of how many symbols can ideally be factored along $L_i$
\end{Def}

Figure~\ref{code:multi_loopnest_a} shows an example of a trivial outer-product loop nest of arity $n=2$ with sharing along dimension $j$ induced by $b_j$.

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 7pt \subfigure[With sharing]{\label{code:multi_loopnest_a}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest.code}}}
~~~~~~~~~~~~~~
{\subfigcapskip = 2pt \subfigure[Optimal form]{\label{code:multi_loopnest_b}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/multilinear_loopnest_opt.code}}}
\end{CenteredBox}\caption{A simple outer-product loop nest. Note that $L^{sf}_i = 2$ and $L^{sf}_j = 1$.}\end{figure}

We prove that outer-product loop nests can be reduced to optimal form. The proof focuses on outer-product loop nests of arity $n=2$ (for $n < 2$ the proof is trivial; for $n > 2$, which is beyond the scope of this paper, a more general treatise would be required to avoid sub-optimal outer loops in pathological cases).

\begin{Prop}
\label{prop:optimal-mln}
Assume $\Lambda = [L_{i_{0}}, L_{i_{1}}]$ is an outer-product loop nest with the body of $L_{i_{1}}$ being an independent basic block. Then an optimal synthesis $\Lambda'$ can be determined by eliminating sharing from both loops, starting from the loop with smallest sharing factor.
\end{Prop}
\begin{proof}
The demonstration is by construction and exploits linearity (recall that outer-product loop nests are multilinear by definition). We start with ``flattening'' the expressions appearing in the body of $L_{i_{1}}$; this is because, depending on the input structure, products might have to be recursively expanded in order to expose the space of all possible factorizations. The expanded expressions are summations of outer products (otherwise, the assumption of $\Lambda$ being an outer-product loop nest would trivially be contradicted).

We traverse the expression tree and determine the sharing factor of the two linear loops. Starting from the loop with smallest sharing factor, $L_i$, we incrementally factorize its symbols. Due to linearity, each factored product $P$ only has one symbol depending on $L_i$, and this symbol is now unique in the expression. The other expression in $P$, independent of $L_i$, is, by definition, loop-invariant, and therefore hoisted such that redundant computation is avoided. This transformation is semantically correct, regardless of $L_i$ being $L_{i_{0}}$ or $L_{i_{1}}$: multilinearity ensures deterministic factorization; perfectness ensures safeness of hoisting (see~\citeN{Luporini} for more details about code motion). We repeat the same process for the loop with higher sharing factor, $L_j$.

We now have to demonstrate that the chosen factorization ordering, based on sharing factors, leads to optimality. This reduces to prove that the two following facts are true
\begin{enumerate}
\item Taking $L_j$, the loop with higher sharing factor, as point of departure leads to a sub-optimal $\Lambda'$ (optimal only in the best case). By contradiction, let us assume this is false. There are two possibilities: A) both $L_i$ and $L_j$ present sharing, but this occurs over disjoint sets of outer products; B) at least one outer product induces sharing along both $L_i$ and $L_j$. Case A) is quite trivial since there are no side effects in swapping the order of factorizations, so $\Lambda'$ is actually optimal (best case scenario). In case B), due to linearity, factorization produces a summation of $L^{sf}_{j}$ outer products, as opposed to $L^{sf}_{i}$ if we had taken $L_i$ as point of departure. Therefore, since $L^{sf}_{j} > L^{sf}_{i}$, the innermost loop results more expensive. We still wonder about the cost of hoisting, since clearly we are hoisting less expressions in this scenario. However, a hoisted symbol always costs one operation (a sum), while a non-factored outer product always costs two operations (a product and a sum). That is, factorization is always convenient. The initial assumption is therefore wrong.
\item Partial elimination of sharing leads to a sub-optimal $\Lambda'$ (optimal only in the best case). By contradiction, let us assume this is false. Because of linearity, we know that factorization eliminates sharing. We wonder what happens in the case of partial factorization; that is, at least one factorizable symbol, say along $L_i$, is left outside of the factored product (e.g., $a_i$ in $a_i(b_j + c_j + ...) + a_i e_j$). It is fairly simple to see that either there is a factorization opportunity along $L_j$, in which case no penalty needs be paid; or $\Lambda'$ is sub-optimal, since eventually the innermost loop will perform more operations. The initial assumption is therefore contradicted. Note that, if we were considering arities $n > 2$, this fact would generally be false.
\end{enumerate}
\end{proof}

For example, by applying this procedure to the code in Figure~\ref{code:multi_loopnest_a}, we obtain the optimal form in Figure~\ref{code:multi_loopnest_b}.
  
We now observe that, for the larger class of finite element integration loop nests, the presence of sharing is a \textit{sufficient but not necessary} condition for being in \textit{non-optimal} form. Consider again the bilinear form implementation in Figure~\ref{code:loopnest}. We pose the following question: are we able to identify sub-expressions within $F$ for which the reduction imposed by $L_i$ can be pre-evaluated, thus obtaining a decrease in operation count proportional to the size of $L_i$, $M$? The transformation we look for is exemplified in Figures~\ref{code:loopnest_red} (input) and~\ref{code:loopnest_nored} (output); Figure~\ref{code:loopnest_red} can be seen as a simple instance of the abstract loop nest in Figure~\ref{code:loopnest}.

\begin{figure}[h]\begin{CenteredBox}
{\subfigcapskip = 19pt \subfigure[With reduction]{\label{code:loopnest_red}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_red.code}}}
~~~~~~~~~~
{\subfigcapskip = 2pt \subfigure[Pre-evaluated reduction]{\label{code:loopnest_nored}\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_nored.code}}}
\end{CenteredBox}\caption{Exposition (through factorization) and pre-evaluation of a reduction.}\end{figure}

Pre-evaluation opportunities could be exposed through an exploration of the expression tree transformation space. For arbitrary loop nests, this can be challenging. Further, the following issues arise when considering pre-evaluation opportunities:
\begin{itemize}
\item as opposed to what happens with hoisting in outer-product loop nests, the temporary variable size is proportional to the number of non-reduction loops crossed (for the bilinear form implementation in Figure~\ref{code:loopnest}, $N \cdot O$ for sub-expressions depending on $[L_i, L_j, L_k]$ and $L \cdot N \cdot O$ for those depending on $[L_e, L_i, L_j, L_k] $). This might shift the loop nest from a CPU-bound to a memory-bound regime, which might be counter-productive for actual runtime
\item the transformations exposing pre-evaluation opportunities could increase the arithmetic complexity (e.g., expansion may increase the operation count; further examples will be provided later). This could overwhelm the gain inherent in pre-evaluation.
\end{itemize}

To summarize, so far we have highlighted four problems:
\begin{enumerate}
\item The need for an algorithm to expose pre-evaluation opportunities
\item Potential explosion in working set size
\item Potential increase in operation count due to manipulation of the expression tree
\item The need for a strategy to coordinate sharing elimination and pre-evaluation opportunities (sharing elimination could prevent pre-evaluation; pre-evaluation might expose further sharing)
\end{enumerate}

We expand on these points in Section~\ref{sec:optimal-synthesis}. In the perspective of addressing point 2), we conclude this section refining our optimality statement as follows.

\begin{Def}[Optimality of a loop nest with bounded working set]
\label{def:optimality}
Let $\Lambda$ be a generic loop nest, and let $\Gamma$ be a generic transformation function $\Gamma : \Lambda \rightarrow \Lambda'$ such that $\Lambda'$ is semantically equivalent to $\Lambda$ (possibly, $\Lambda' = \Lambda$) and a set of memory constraints is satisfied. We say that $\Lambda' = \Gamma (\Lambda)$ is an optimal synthesis of $\Lambda$ if the number of operations that it performs to evaluate the result is minimal.
\end{Def}

\section{Transformation Algorithm}
\label{sec:optimal-synthesis}

In this section, we build a transformation algorithm capable of reducing finite element integration loop nests to optimal form. A fundamental aspect is the definition of a suitable cost function to assess the profitability of pre-evaluation, the transformation introduced in the previous section (and later further expanded). The optimality of the transformation algorithm is discussed.

\subsection{Memory constraints}
\label{sec:mem-const}
We provide intuitions about the need for memory constraints. Our point of departure is again the bilinear form implementation in Figure~\ref{code:loopnest}. Analogous considerations apply to forms of arbitrary arity.

The fact that $L \gg M, N, O$ suggests we should be cautious about hoisting mesh-dependent (i.e., $L_e$-dependent) expressions. Imagine $\Lambda$ is enclosed in a time stepping loop. One could think of exposing (through some transformations) and hoisting any time-invariant sub-expressions to minimize redundant computation at every time step. The working set size could then increase by a factor $L$. The gain in number of operations executed could therefore be overwhelmed, from a runtime viewpoint, by a much larger memory pressure.

A second, more general observation is that, for certain forms and discretizations, aggressive hoisting can make the working set exceed the size of ``some level of local memory'' (e.g. the last level of private cache on a conventional CPU, the shared memory on a GPU). For example, pre-evaluating geometry-independent expressions outside of $\Lambda$ requires temporary arrays of size $N\cdot O$ for bilinear forms and of size $N$ (or $O$) for linear forms. This can sometimes break a ``local memory threshold''. We will report results of our experiments on the memory threshold in Section~\ref{sec:perf-results-forms}.

Based on these considerations, we establish the following set of memory constraints
\begin{enumerate}
\item The size of a temporary due to code motion cannot be larger than that of the outer-product loop nest iteration space. 
\item The total amount of memory occupied by the temporaries created by code motion cannot exceed a threshold \texttt{$T_H$}
\end{enumerate}
A corollary of constraint 1) is that hoisting expressions involving geometry-dependent terms outside of $\Lambda$, which is a possibility we discussed at the beginning of this section, is now forbidden. This clearly shrinks the transformation space, pruning points that most likely would result in sub-optimal performance.

\subsection{Minimization of the Operation Count}
We here discuss how we can systematically reach the goal set by Definition~\ref{def:optimality}.

We reinforce that eliminating sharing from the outer-product loop nest does not suffice. As suggested in Section~\ref{sec:optimal-impl}, we wonder whether, and under what conditions, the reduction imposed by $L_i$ could be pre-evaluated, thus reducing the operation count. 

To partly answer this question, we make use of a result -- the foundation of tensor contraction mode -- from~\citeN{Kirby:TC}. Essentially, multilinear forms can be seen as sums of monomials, each monomial being an integral over the equation domain of products (of derivatives) of functions from discrete spaces; such monomials can always be reduced to a product of two tensors (see Section~\ref{sec:background}). We interpret this result at the loop nest level: with an input as in Figure~\ref{code:loopnest}, we can always dissect \texttt{F} into distinct sub-expressions (the monomials). Each sub-expression is factorizable so as to split constants from $[L_i, L_j, L_k]$-dependent terms. These $[L_i, L_j, L_k]$-dependent terms can be hoisted outside of $\Lambda$, and finally pre-evaluated into temporaries. As part of this process, the reduction induced by $L_i$ is computed and, therefore, disappears from $\Lambda$. This is actually what we were referring to when we introduced the pre-evaluation transformation in Section~\ref{sec:optimal-impl}. 

There are two issues: understanding for which monomials pre-evaluation is profitable; coordinating pre-evaluation with sharing elimination, since the latter prevents the former. We start with the second issue. For this, it is convenient to introduce the transformation algorithm. Figure~\ref{code:intuition} provides the pseudo-code of the algorithm. An explanation and a discussion on its optimality follow. 

\begin{figure}[h]
\begin{lstlisting}[numbers=left, basicstyle=\small\ttfamily, frame=single]
dissect the input expression into monomials
for each monomial M:
  $\theta_{w}$ = estimate operation count with pre-evaluation
  $\theta_{wo}$ = estimate operation count without pre-evaluation
  if $\theta_{w}$ < $\theta_{wo}$ and memory constraints satisfied:
    mark M as candidate for pre-evaluation
for each monomial M:
  if M does not share terms with M$'$, an unmarked monomial:
    extract M into a separate loop nest
    apply pre-evaluation to M
for each expression:
  remove sharing    
\end{lstlisting}
\caption{Intuition of the transformation algorithm}
\label{code:intuition}
\end{figure}

We study the impact of pre-evaluation, as number of operations saved or introduced, ``locally''; that is, for each monomial, in isolation. If we estimate that, for a given monomial, pre-evaluation will decrease the operation count, then the corresponding sub-expression is extracted, a sequence of transformation steps -- involving expansion, factorization, code motion -- takes place (details in Section~\ref{sec:codegen}), and the evaluation eventually performed. The result is a set of $n$-dimensional tables (these can be seen as ``slices'' of the reference tensor at the math level), $n$ being the arity of the multilinear form. Identical tables are mapped to the same temporary. Eventually, sharing is removed from the resulting expressions by applying a procedure as described in Proposition~\ref{prop:optimal-mln}. The transformed loop nest is as in Figure~\ref{code:loopnest-opt}. 

\begin{figure}\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_opt.code}
\end{CenteredBox}\caption{A possible loop nest for the local assembly of a form after applying pre-evaluation to some of the monomials (resulting in sub-expressions \texttt{G'}, \texttt{G''}, ...)}\label{code:loopnest-opt}\end{figure}

Before elaborating on the profitability of pre-evaluation (i.e., how to determine the cost function $\theta$ in Figure~\ref{code:intuition}), we need to discuss under which conditions this approach, based on a ``local analysis'' of monomials, is able to reduce loop nests to optimal form. 

\begin{Prop}
\label{prop:optimal-approach}
Consider an integration routine originating from an arbitrary multilinear form. A loop nest $\Lambda$, as previously characterized, evaluates an expression comprising a set of monomials, $M$. Let $P$ be the set of monomials for which pre-evaluation is applied because found profitable, and let $Z$ be the set of non pre-evaluated monomials (i.e., $Z = M \setminus P$). Assume that:
\begin{enumerate}
\item the cost function $\theta$ employed is optimal; that is, it predicts correctly whether pre-evaluation is profitable or not for a monomial
\item pre-evaluating different monomials does not result in identical tables
\item monomials in $P$ do not share terms
\end{enumerate} 
Then, the main algorithm in Figure~\ref{code:intuition} leads to inner optimality under the set of memory constraints $C$ (defined in Section~\ref{sec:mem-const}).
\end{Prop}
\begin{proof}
We first comment on the assumptions. 1) We postpone the discussion of how to build $\theta$ to Section~\ref{sec:op_count}. 2) Identical pre-evaluated tables from distinct monomials could be the result of symmetries in tabulated basis functions. This is however a pathological case that we can harmlessly ignore. 3) In complex forms with several monomials, different pre-evaluation candidates could actually share terms. We abstract from this case, which otherwise would require a ``global'' analysis of the monomials in the form that we believe would not justify the gain (if any) in operation count. 

We distinguish two classes of loop nests rooted in $\Lambda$: $[L_e, L_j, L_k]$, for the pre-evaluated monomials in $P$, and $[L_e, L_i, L_j, L_k]$, enclosing the remaining monomials in $Z$. Since they only differ for the presence of $L_i$, we relieve the notation by omitting the shared loops when discussing operation counts. The operation count of what we are proving to be the optimal $\Lambda$ synthesis (inferable from Figure~\ref{code:loopnest-opt}) is $\Lambda_{ops} = \Lambda_{ops_1} + \Lambda_{ops_2} = \sum_{\alpha}^{\# P} p_{\alpha} + I \sum_{\beta}^{\# Z} z_{\beta}$, where $p_{\alpha}$ and $z_{\beta}$ represent the operation count of monomials in $P$ and $Z$, respectively, while $I$ is the iteration space size of $L_i$.

We note that, as explained in Section~\ref{sec:mem-const}, $C$ imposes constraints on hoisting. This narrows the proof to demonstrating the following: A) pre-evaluating any $Z_P: Z_P \subseteq Z$ would increase $\Lambda_{ops}$; B) not pre-evaluating any $P_Z: P_Z \subseteq P$ would increase $\Lambda_{ops}$.

A) We prove that $\Lambda_{ops}' = \Lambda_{ops_1}' + \Lambda_{ops_2}' > \Lambda_{ops}$. It is rather obvious that $\Lambda_{ops_1}' \geq \Lambda_{ops_1}$ (it is equal only if, trivially, $Z_P = \emptyset$). We note that if monomials in $Z_P$ share terms with $\overline{Z} = Z \setminus Z_P$, then we have $\Lambda_{ops_2}' = \Lambda_{ops_2}$, so our statement is true. If, on the other hand, at least one monomial does not share any terms, we obtain $\Lambda_{ops_2}' < \Lambda_{ops_2}$ or, equivalently, $\Lambda_{ops_2}' = \Lambda_{ops_2} - I \cdot\delta$. What we have to show now is that even by exposing more pre-evaluations, $\Lambda_{ops_1}' - \Lambda_{ops_1} =  \gamma \geq I \cdot\delta$ holds. Because of assumption 2), the new pre-evaluations cannot expose further sharing. Therefore, the optimality of the cost function (assumption 1) ensures our claim holds.

B) In absence of sharing, the statement is trivially true since we would have $\Lambda_{ops_2}' > \Lambda_{ops_2}$, being the cost function optimal due to assumption 1). Assumption 3) guarantees there cannot be sharing within $P_Z$, which avoids subtle cases in which pre-evaluation would be sub-optimal due to destroying sharing-removal opportunities. The last case we have to consider is when $p \in P_Z$ shares at least one term with $z \in Z$. This situation cannot actually occur by construction: all candidates for pre-evaluation sharing terms with monomials in $Z$ are ``de-classed'' from $P$ to $Z$ (see Figure~\ref{code:intuition}, line 8). The rationale is that since we would have to pay anyway the presence of $z$ in the innermost loop, adding $p$ to $Z$ would not augment the operation count in our model, so we can safely avoid pre-evaluation. 

% assunzione 3 è quella che previene la somma di due mass matrix moltiplicate da F0 una e da F1 l'altra...ma non solo ! chesso magari la trial function è derivata in un monomial e nell'altra no, ma la test function è sempre uguale...
\end{proof}

\subsection{Cost Function}
\label{sec:op_count}
It remains to tie one loose hand: the construction of the pre-evaluation cost function $\theta$. We define it such that $\theta : M \to \mathbb{N} \times \mathbb{N}$; that is, given a monomial, two natural numbers representing the sharing-free operation count without ($\theta_{wo}$) and with ($\theta_{w}$) pre-evaluation are returned. Since $\theta$ is expected to be used by a compiler to drive the transformation process, requirements are simplicity and velocity.  

We can easily predict $\theta_{wo}$ thanks to our key property, linearity. This was explained in Proposition~\ref{prop:optimal-mln}. A simple analysis suffices to obtain the cost of a  sharing-free multilinear loop nest, namely $\Lambda_{ops}^{ns}$. Assuming $I$ to be the size of the $L_i$ iteration space, we have that $\theta_{wo} = \Lambda_{ops}^{ns} \cdot I$.

For $\theta_{w}$, things are more complicated. We first need to estimate the \textit{increase factor}, $\iota$, to account for the presence of (derivatives of) coefficients. This number captures the increase in arithmetic complexity due to the transformations enabling pre-evaluation. To contextualize, consider the example in Figure~\ref{code:increase_factor}. 

\begin{figure}[h]\begin{CenteredBox}
\lstinputlisting[basicstyle=\footnotesize\ttfamily]{listings/loopnest_inc_factor.code}
\end{CenteredBox}\caption{Simplified loop nest for the local assembly of a pre-multiplied mass matrix.}\label{code:increase_factor}\end{figure}

One can think of this as the (simplified) loop nest originating from the assembly of a pre-multiplied mass matrix. The sub-expression \texttt{f[0]*B[i,0]+f[1]*B[i,1]+f[2]*B[i,2]} represents the field $f$ over (tabulated) basis functions $B$. In order to apply pre-evaluation, the expression needs be transformed to separate $f$ from the integration-dependent (i.e., $L_i$-dependent) quantities. By expanding the product we observe an increase in the number of $[L_j, L_k]$-dependent operations of a factor 3 (the local degrees of freedom for the coefficient). Intuitively, $\iota$ captures this growth in non-hoistable operation. 

With a single coefficient, as we just saw, $\iota$ directly descends from the cost of expansion. In general, however, predicting $\iota$ is less straightforward. For example, consider the case in which a monomial has multiple coefficients expressed over the same function space. The expansion would now lead to identical sub-expressions that, once pre-evaluated, would be mapped to the same temporary. The resulting loop nest would be characterized by sharing, as a result. Therefore, the actual operation count (i.e., once sharing is removed) would be smaller than that one could infer from analysing the expansion ``in isolation''. For a precise estimate of $\iota$, we then need to calculate the $k$-combinations with repetitions of $n$ elements, with $k$ being the number of coefficient-dependent terms appearing in a product (in the example, there is only $f$, so $k=1$)  and $n$ the cardinality of the set of symbols involved in the coefficient expansion (in the example, \texttt{B[i,0]}, \texttt{B[i,1]}, and \texttt{B[i,2]}, so $n=3$; note that we are talking about sets here, so duplicates would be counted once). 

If $\iota \geq I$ we can immediately say that pre-evaluation will not be profitable. This is indeed a necessary condition that, intuitively, tells us that if we add to the innermost loop more operations than we actually save from eliminating $L_i$, then for sure $\theta_{wo} < \theta_{w}$. This observation can speed up the compilation time by decreasing the analysis cost.

If, on the other hand, $\iota < I$, a further step is necessary to estimate $\theta_{w}$. In particular, we need to calculate the number of terms $\rho$ such that $\theta_{w} = \rho \cdot \iota$. Consider again Figure~\ref{code:increase_factor}. In the case of the mass matrix, the body of $L_k$ is simply characterized by the dot product of test and trial functions, \texttt{B[j]*B[k]}, so trivially $\rho = 1$. In general, $\rho$ varies with the discretization and the differential operators used in the form. For example, in the case of the bilinear form originating from a standard bi-dimensional Poisson equation, the reader could verify that after a suitable factorization we would have $\rho = 3$. There are several ways of determining $\rho$. The fastest would be to extract it from high-level analysis of form and discretization; for convenience, in our implementation we have algorithms that, based on analysis of the expression tree, project the output of monomial expansion and factorization, which in turn gives us $\rho$.

% apply memory constraints: reduction to knapsack problem ... not implemented, we use greedy


\section{Code Generation}
\label{sec:codegen}
The model described in Section~\ref{sec:optimal-synthesis} has been fully automated in COFFEE, the optimizer of finite element integration routines used in Firedrake. In this section, we describe the features of this code generation system.

\subsection{Automation through the COFFEE Language}
As opposed to what happens in the FEniCS Form Compiler with quadrature and tensor modes, there are no separate trunks in COFFEE handling pre-evaluation, sharing, and code motion in general. All optimizations are instead expressed as composition of parametric ``building-block'' operations. This has several advantages. Firstly, extendibility: novel transformations -- for instance, sum-factorization in spectral methods -- could be expressed using the existing operators, or with small effort building on what is already available. Secondly, generality: other domains sharing properties similar to that of finite element integration (e.g., multilinear loop nests) could be optimized through the same compiler. Thirdly, robustness: the same building-block operations are exploited, and therefore stressed, by different optimization pipelines.

A non-exhaustive list of such operations includes expansion, factorization, re-association, generalized code motion. These ``rewrite operators'' can be seen as the COFFEE language. They define parametric transformations: for example, one could ask to factorize constant rather linear terms, while hoisting could be driven by loop dependency. Their implementation is based on manipulation of the abstract syntax tree representing the integration routine. 

\subsubsection{Heuristic Optimization of Integration-dependent Expressions}
\label{sec:heuristic-int-dep}
As a proof-of-concept of our generality claim, we briefly discuss our optimization strategy for integration-dependent expressions. These are expressions that should logically be placed within $L_i$. They can originate, for example, from the extensive use of tensor algebra in the derivation of the weak variational form or from the use of a non-affine reference-to-physical element mapping, which Jacobian needs be re-evaluated at every quadrature point. For some complex monomials and for coarser discretizations, the operation count within $L_i$ could be comparable or, in some circumstances, even outweigh that of the multilinear loop nest. In these cases, our optimality model becomes weaker, since its underlying assumption is that the bulk of the computation is carried out in innermost loops.

Despite the fact that we are not characterizing optimality for this wider class of problems, we can still heuristically apply the same reasoning of Sections~\ref{sec:optimal-impl} and~\ref{sec:optimal-synthesis} to try to eliminate sharing, thus reducing the operation count. This is straightforward in our code generation system by composing rewrite operators. Our strategy is as follows. 

COFFEE is agnostic with respect to the high level form compiler, so the first step consists of removing redundant sub-expressions. This is because a form compiler abstracting from optimization will translate expressions as in Equation~\ref{eq:quadrature} directly to code without performing any sort of analysis. Eliminating redundant sub-expressions is usually helpful to relieve the arithmetic pressure inherent in $L_i$. We then synthesize an optimal loop nest as described in the previous sections. This may in turn expose a set of $L_i$-dependent expressions. For each of this expressions, we try to remove sharing by greedily applying factorization and code motion. In the COFFEE language, this process is expressed by simply composing five rewrite operators.

\subsection{Low-level Optimization}
\label{sec:code-spec}
We comment on a set of low-level optimizations. These are essential to 1) achieve machine peak performance (Sections~\ref{sec:opt-review} and~\ref{sec:vect-prom}) and 2) make COFFEE independent of the high-level form compiler (Section~\ref{sec:zeros}). As we will see, there are interplays among different transformations. For completeness, we present all of the transformations available in the compiler, although we will only use a subset of them for a fair performance evaluation.

\subsubsection{Review of Existing Optimizations}
\label{sec:opt-review}
We start with briefly reviewing the low level optimizations presented in~\citeN{Luporini}.

\paragraph{Padding and data alignment} All of the arrays involved in the evaluation of the local element matrix or vector are padded to a multiple of the vector register length. This is a simple yet powerful transformation that maximizes the effectiveness of vectorization. Padding, and then loop bounds rounding, enable data alignment and avoid the introduction of scalar remainder loops. 

\paragraph{Vector-register Tiling} Blocking (or tiling) at the level of vector registers improves data locality beyond traditional unroll-and-jam transformations. This blocking strategy consists of evaluating outer products by using just two vector register and without ever spilling to cache. 

\paragraph{Expression Splitting} When the number of basis functions arrays (or, equivalently, temporaries introduced by code motion) and constants is large, the chances of spilling to cache are high in architectures with a few logical registers (e.g. 16/32). By exploiting sum’s associativity, an expression can be fissioned so that the resulting sub-expressions can be computed in separate loop nests. This reduces the register pressure. 

\subsubsection{Vector-promotion of Integration-dependent Expressions}
\label{sec:vect-prom}
Integration-dependent expressions are inherently executed as scalar code because vectorization (unless employing special hand-written schemes) occurs along a single loop, typically the innermost. For the same reasons discussed in Section~\ref{sec:heuristic-int-dep}, we also want to vectorize along $L_i$. One way to achieve this is vector-promotion. This requires creating a ``clone'' of $L_i$ in the preheader of the loop nest, in which vector temporaries are evaluated in what is now an innermost loop. 

\subsubsection{Handling Sparse Tables}
\label{sec:zeros}
Consider a set of tabulated basis functions with quadrature points along rows and functions along columns. For example, \texttt{A[i,j]} provides the value of the \texttt{j-th} basis function at quadrature point \texttt{i}. Unless using a smart form compiler (which we want to avoid), there are circumstances in which the tables are sparse. Zero-valued columns arise when taking derivatives on a reference element or when employing vector-valued elements. Zero-valued rows can result from using non-standard functions spaces, like Raviart-Thomas. Zero-valued blocks can appear in pre-evaluated temporaries. Our objective is a transformation that avoids useless iteration over zeros while preserving the effectiveness of the other low-level optimizations, especially vectorization. 

In~\citeN{quadrature1}, a technique to avoid iteration over zero-valued columns based on the use of indirection arrays (e.g. \texttt{A[B[i]]}, in which \texttt{A} is a tabulated basis function and \texttt{B} a map from loop iterations to non-zero columns in A) was proposed. Our approach, which will be compared to this pioneering work, aims to free the generated code from such indirection arrays. This is because we want to avoid non-contiguous memory loads and stores, which can nullify the benefits of vectorization. 

The idea is that if the dimension along which vectorization is performed (typically the innermost) has a contiguous slice of zeros, but that slice is smaller than the vector length, then we do nothing (i.e., the loop nest is not transformed). Otherwise, we restructure the iteration space. This has several non-trivial implications. The most notable one is memory offsetting (e.g., \texttt{A[i+m,j+n]}), which dramatically enters in conflict with padding and data alignment. We use heuristics to retain the positive effects of both and to ensure correctness. Details are, however, beyond the scope of this paper. 

The implementation is based on symbolic execution: the loop nests are traversed and for each statement encountered the location of zeros in each of the involved symbols is tracked. Arithmetic operators have a different impact on tracking. For example, multiplication requires computing the set intersection of the zero-valued slices (for each loop dimension), whereas addition requires computing the set union.

% say that the reasons to have zeros is to keep the design of the form compiler simple, which just ``puts zeros'' and ``outline the assembly expression''.
% rationale: contiguous in the innermost dimension are aggregated. contiguous in all other dimensions are not, unless complete overlap.
% tables shrunk and merged

\section{Performance Evaluation}
\label{sec:perf-results}

\subsection{Experimental Setup}

Experiments were run on a single core of an Intel I7-2600 (Sandy Bridge) CPU, running at 3.4GHz, 32KB L1 cache (private), 256KB L2 cache (private) and 8MB L3 cache (shared). The Intel Turbo Boost and Intel Speed Step technologies were disabled. The Intel \texttt{icc 15.2} compiler was used. The compilation flags used were \texttt{-O3, -xHost, -ip}, which can trigger \texttt{AVX} autovectorization. 

We analyze the runtime performance in four real-world bilinear forms of increasing complexity, which comprise the differential operators that are most common in finite element methods. In particular, we study the mass matrix (``\texttt{Mass}'') and the bilinear forms arising in a Helmholtz equation (``\texttt{Helmholtz}''), in an elastic model (``\texttt{Elasticity}''), and in a hyperelastic model (``\texttt{Hyperelasticity}''). The complete specification of these forms is made publicly available\footnote{\url{https://github.com/firedrakeproject/firedrake-bench/blob/experiments/forms/firedrake_forms.py}}. 

We evaluate the speed-ups achieved by a wide variety of transformation systems over the original code (i.e., no optimizations applied) as returned by the FEniCS Form Compiler. We analyze the impact of
\begin{itemize}
\item FEniCS Form Compiler: optimized quadrature mode (work presented in~\citeN{quadrature1}). Referred to as \texttt{quad} 
\item FEniCS Form Compiler: tensor mode (work presented in~\citeN{FFC-TC}). Referred to as \texttt{tens} 
\item FEniCS Form Compiler: automatic mode (choice between \texttt{tens} and \texttt{quad} driven by heuristic, detailed in~\citeN{Fenics} and summarized in Section~\ref{sec:qualitative}). Referred to as \texttt{auto} 
\item UFLACS: a novel back-end for the FEniCS Form Compiler (whose primary goals are improved code generation time and runtime). Referred to as \texttt{ufls} 
\item COFFEE: generalized loop-invariant code motion and padding (work presented in~\citeN{Luporini}). Referred to as \texttt{cfO1} 
\item COFFEE: optimal multilinear loop nest synthesis, padding and symbolic execution (captures the contributions of this paper). Referred to as \texttt{cfO2} 
\end{itemize}

The values that we report are the average of three runs with ``warm cache'' (no code generation time, no compilation time). They include the cost of local assembly as well as the cost of matrix insertion. However, the unstructured mesh used to run the simulations was chosen small enough to fit the L3 cache of the CPU, so as to minimize the ``noise'' due to operations outside of the element matrix evaluation. 

For the fairest comparison possible, small patches (publicly available) were written to be able to run all simulations through Firedrake: this means the cost of matrix insertion and mesh iteration is exactly the same in all variants. UFLACS and the FEniCS Form Compiler's optimization systems generate code suitable for FEniCS, which employs a data storage layout different than Firedrake. Our patches fix this problem by producing code with a data storage layout as expected by Firedrake. 

For what concerns the memory constraint $C_2$ introduced in Section~\ref{sec:mem-const}, we set $T_H = L2_{size}$; that is, the amount of space that temporaries due to pre-evaluation can occupy is bounded by the size of the processor L2 cache (the last level of private cache). For the test cases in which $T_H$ was determinant to prevent pre-evaluation, we also repeated the experiments setting $T_H = L3_{size}$ to assess our hypotheses. Results of these trials are discussed below.

Following the methodology adopted in~\citeN{quadrature1}, we increasingly vary the complexity of each form. In particular:
\begin{itemize}
\item the polynomial order of test and trial functions, $q \in \lbrace1, 2, 3, 4\rbrace$. Test and trial functions always have same degree
\item the polynomial order of coefficient (or ``pre-multiplying'') functions, $p \in \lbrace1, 2, 3, 4\rbrace$
\item the number of coefficient functions $nf \in \lbrace0, 1, 2, 3\rbrace$
\end{itemize}
Constants of our analysis, instead, are
\begin{itemize}
\item the space of test, trial, and coefficient functions: Lagrange
\item the mesh: tetrahedral with a total of 4374 elements
\item exact numerical quadrature (the same scheme as in~\citeN{quadrature1}, based on the Gauss-Legendre-Jacobi rule, is employed)
\end{itemize}

The upcoming Figures (\ref{fig:mass},~\ref{fig:helmholtz},~\ref{fig:elasticity}, and~\ref{fig:hyperelasticity}) can be interpreted as ``nested'' plots. We refer to the outer plot as the ``grid'', while the inner are the actual ``plots''. In a grid, $p$ varies along the horizontal axis and $q$ varies along the vertical axis. The top-left plot in a grid shows the speed-up over original code for $[q=1, p=1]$; the plot on its right for $[q=1, p=2]$, and so on. The diagonal of the grid provides the behaviour when test, trial and coefficient functions have same polynomial order, that is $q=p$. A grid can therefore be looked at from different perspectives, which allows us to make structured considerations on the performance achieved. In each plot there are six groups of bars, each group referring to a particular code variant (\texttt{quad, tens, ...}). There are four bars per group: the leftmost bar corresponds to the case $nf = 0$, the one on its right to the case $nf = 1$, and so on. 

%TODO : can we just say padding is useless because in 3D ndofs is a multiple of our vector length?

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/mass}}
\caption{Performance evaluation for the mass matrix. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:mass}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/helmholtz}}
\caption{Performance evaluation for the bilinear form of a Helmholtz equation. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:helmholtz}
\end{figure}

\begin{figure}
\includegraphics[scale=0.7]{perf-results/elasticity}
\caption{Performance evaluation for the bilinear form arising in a elastic model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:elasticity}
\end{figure}

\begin{figure}
 \makebox[\textwidth][c]{\includegraphics[scale=0.77]{perf-results/hyperelasticity}}
\caption{Performance evaluation for the bilinear form arising in a hyperelastic model. The bars represent speed-up over the original (unoptimized) code produced by the FEniCS Form Compiler.}\label{fig:hyperelasticity}
\end{figure}

\subsection{Performance of Forms}
\label{sec:perf-results-forms}
% 52 test cases per form, so 52*4 = 208 test cases in total. In 7, there is a non-marginal slow down in cfO2. 

The first observation is that our optimality model does not imply minimum execution time. In particular, in the 0.06$\%$ of the test cases (we are not counting marginal differences), \texttt{cfO2} does not result in the best runtime. There are two reasons for this. The former is that low level optimization can have a significant impact. Compare, as an example, \texttt{tens} and \texttt{cfO2} in \texttt{Mass} and \texttt{Helmholtz} when $q=1$. With $p \in [2,3]$, \texttt{tens} slightly outperforms \texttt{cfO2}. With $p=4$, \texttt{tens} is quite faster than \texttt{cfO2}. The motivation is the aggressive unrolling performed in \texttt{tens}, which 1) allows to eliminate all operations involving zero-valued entries and 2) can drastically decrease both working set and register pressure by avoiding storing entire temporaries (there are often many repeated values that, with unrolling, need be allocated only once). In COFFEE we never unroll loops to maximize the chances of autovectorization, which usually results in considerable speed-ups throughout the majority of test cases. The latter reason is simply inherent to our optimality model. As explained in Section~\ref{sec:heuristic-int-dep}, we use heuristics to optimize the integration loop. The reason we are studying an hyperelastic model is indeed to assess their effectiveness; the systematic performance improvements observed over all other variants are, however, extremely encouraging. 

The second aspect regards the ``triggering'' of pre-evaluation in \texttt{cfO2}. The fact that a bar in \texttt{cfO2} matches the corresponding bar in \texttt{cfO1} usually suggests that pre-evaluation was found unprofitable. The trend -- although there are quite a few exceptions -- is that as $nf$ increases, pre-evaluation tends not to be performed. This is a consequence of a larger increase factor, as discussed in Section~\ref{sec:op_count}.

The third point concerns the effect of the chosen discretizations on autovectorization. The discretizations employed ensure both multilinear loop nest and tabulated basis function sizes to be multiple of the machine vector length. Given the suitable choice of compilation options, this promotes autovectorization in the majority of code variants. The biggest exception is \texttt{quad}, in which, due to the presence of indirection arrays in the generated code, vectorization is rarely triggered. In \texttt{tens}, the loop nests are fully unrolled, so the standard loop vectorization is simply not possible; however, manual inspection of the compiled code suggests block vectorization (\cite{SLP-vect}) is often triggered. In \texttt{ufls}, \texttt{cfO1}, and \texttt{cfO2} the iteration spaces tend to have the same ``structure'' (there are very few exceptions due to low level optimization), with loop vectorization being regularly applied, as far as we could evince. It is worth noting the fact that the size of loops and tabulated basis functions is a multiple of the vector length render padding and data alignment (in \texttt{cfO1} and \texttt{cfO2}), to the best of our understanding, basically irrelevant. 

Also note some other minor things.
\begin{itemize}
\item The lack of (or marginal) improvements in all code variants when $[q=1, p=1]$ is due to the cost of matrix insertion, which outweighs that of local assembly. 
\item The slow-downs experienced with \texttt{quad} are imputable to the presence of indirection arrays in the generated code; especially with dense tables, trading vectorization for avoiding iteration over a few zero-valued columns may be counter-productive (see also Section~\ref{sec:zeros}).
\item A missing bar in \texttt{tens} or \texttt{auto} means that the code generation system failed because of either exceeding the memory limit or being unable to manipulate the math characterizing the form (this happens systematically in the hyperelastic model, and for some discretizations in the elastic model).
\item The bars for $nf=0$ do not change as $p$ increases: this is expected since varying $p$ when there are no coefficients should not make any difference.
 \end{itemize}

\paragraph{Mass}
From analysis of Figure~\ref{fig:mass}, we can see that \texttt{cfO2} is the best variant among the ones tested, apart from a few points already discussed. This suggests that the optimality model holds in these simple bilinear forms; that is, \texttt{cfO2} generates loop nests that are optimal from both a theoretical and practical viewpoints. It is worthwhile noting how \texttt{auto} is not capable of selecting the proper synthesis in many of the test cases, especially as $nf \geq 2$. The fact that \texttt{cfO2} is slower than \texttt{tens} at $[q=4, p=4, nf=1]$ is explained by the memory threshold that prevents pre-evaluation. By just setting $T_H = L3_{size}$, \texttt{cfO2} becomes 1.48$\times$ faster than \texttt{tens}. This is probably due to the hardware prefetching being way more effective than in all other forms: the number of allocated temporaries is relatively small, and so is the number of memory access traces that need be tracked. However, as we already explained, in a parallel context this gain might diminish, since the L3 cache is shared by the cores of the CPU. 

\paragraph{Helmholtz}
Figure~\ref{fig:helmholtz} shows that the performance improvements achieved by various code variants over a non-optimized implementation can be significant. We appreciate the fact that the relative order of the employed function spaces plays a key role in the code synthesis choice. Take, for example, the case $[q=3, p=1, nf=2]$. The adoption of pre-evaluation makes a tremendous speed-up be achievable by \texttt{cfO2} over the competitors. On the other hand, \texttt{auto} triggers \texttt{quad} (since one monomial in the form has two derivatives and two coefficients, which exceeds the threshold within which \texttt{tens} is selected), which, just like the other quadrature-based variants \texttt{ufls} and \texttt{cfO1}, is sub-optimal. The effects of the working set threshold are in general positive. Moving to $T_H = L3_{size}$ makes \texttt{cfO2} slower than \texttt{cfO1} in many cases with $nf=2$, the most significant one being a slow-down of 2.16$\times$ at $[q=2, p=2]$. 

\paragraph{Elasticity}
The performance results for the elastic model are displayed in Figure~\ref{fig:elasticity}. The fact that \texttt{auto} opts for \texttt{tens} when $nf=1$ leads, generally, to sub-optimal execution times. This is different than in the case of the Helmholtz equation, in which the choice of \texttt{tens} was generally correct when $nf=1$. Pre-evaluation is never triggered in \texttt{cfO2}, because of either being estimated sub-optimal or exceeding $T_H$. In the former case, the performance improvements exhibited by \texttt{cfO2} over \text{cfO1} (and, in general, over all code variants) are due to the elimination of sharing and the avoidance of iteration over zero-valued blocks. The latter case occurs with $q=4$ and $nf \in [0, 1]$. Running the same experiments with $T_H = L3_{size}$ resulted in a considerable improvement when $nf = 0$, with \texttt{cfO2} becoming even faster than \texttt{tens}, while slow-downs characterized the cases of $nf=1$ for all values of $p$. Our explanation for this is that when $[q=4, nf=0]$, $T_H$ is exceeded by only 40$\%$, while the cost function predicts a save in operation count of 5$\times$. This suggests that our model could be refined to handle the cases in which the gain in operation count is so large that non-extreme variations in working set size should be considered negligible.

\paragraph{Hyperelasticity}
In the hyperelastic model experiments, pre-evaluation is never triggered by \texttt{cfO2}. This is a consequence of the form complexity, which makes the increase factor overwhelm any potential flop reduction. Another distinguishing aspect is the use of vector-valued function spaces, which requires a technique as in Section~\ref{sec:zeros} to avoid wasteful operations over zero-valued entries; \texttt{quad}, \texttt{tens}, \texttt{ufls} and \texttt{cfO2} employ different techniques. Results are displayed in Figure~\ref{fig:hyperelasticity}. \texttt{cfO2} is the best alternative due to removing sharing from the multilinear loop nest and optimization of integration-dependent expressions. \texttt{ufls} performs generally well and exhibits significant speed-ups over non-optimized syntheses; this is a result of the effort in optimizing integration-dependent expressions. 




\section{Conclusions}
\label{sec:conclusions}
With this research we have made a first step towards producing a theory and an automated system for the optimal synthesis of loop nests arising in finite element integration. The results are extremely encouraging, suggesting our model applies to a variety of contexts. We have discussed the conditions under which the model only leads to quasi-optimal loop nests. An open problem is understanding how to refine this model to include outer loops. This will probably require exploiting mathematical properties of differential operators. A second open problem is extending our methodology to classes of loops arising in spectral methods; here, the interaction with low level optimization will probably become stronger due to the typically larger working sets deriving from the use of high order function spaces. Lastly, we recall our work is publicly available and is already in use in the latest version of the Firedrake framework.

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{biblio}


\medskip

\end{document}
