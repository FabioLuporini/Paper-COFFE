First of all, we would like to thank the referees for their insightful comments,
which helped improving the paper. In this new version, we have attempted to
address all of the referees' concerns. We explain how the paper has changed
below, by considering each of the requests/observations/questions raised by the
three referees individually.

Referee: 1
==========

| This paper is overall well written. Examples are very illustrative and help
| understand the article. My only concern is about how authors relates their
| theortical results with numerical performance examples. [...]

| My suggestions are as follows:
| 1. It would be better to include actual transformed code and original code in
| the appendix for all computing model (mass, helmholtz, elasticity, hyper
| elasticity). At least I wonder what makes hugh speed up in the hyper elastic
| problem compared to usual mass matrix.

The generated code has been uploaded to GitHub and permanently
archived with a DOI through Zenodo.  This is properly documented in
the paper (see Section 7.1 - Experimental setup).  For reasons of
space, it simply was not possible to append code in an appendix.

| 2. Please include actual FLOP measurements of input code and transformed code
| and show some correlation with performance.

There is a whole new section treating this fundamental aspect (7.8 - On the
correlation between operation count and performance). Further, a new figure was
added (Fig. 11): this clearly shows the relationship between the achieved
speed-ups and FLOPs reductions. We believe that this figure is particularly
useful, because it shows how well the system presented in the paper scales well
with the complexity of the operators. It also provides a nice summary of the
performance improvements achieved. Moreover, Sections 7.4 through 7.7 (Mass,
Helmholtz, ...) have been extended and now also discuss the impact of the reduced
operation counts in a set of relevant test cases.

Referee: 2
==========

| The paper is well written, interesting does clearly merit publication.
| The usage of advanced compiler technology as demonstrated here is clearly
| novel and it brings accelerated performance.

| There is one issue that is not covered in the analysis and only briefly
| mentioned in the general discussion: the transfer from memory. I understand
| that it complicates the analysis but the least it should be quantified in
| the numerical experiments [...] It is to a certain extent discussed already
| in the paper but it should also be quantified.

Sections 7.4 through 7.7 now discuss the memory transfer cost, but
only in a limited set of test cases. We believe that it is very
difficult to thoroughly study this problem in a paper like ours, being
based on a high dimensional experimentation space. A complete
treatment would probably require a new paper, introducing specific
metrics and showing different angles of the performance achieved.
However, with other coauthors we have very recently published a paper
including exactly the type of analysis advocated by the referee:
[Bercea et al.  2016]. This is now referenced in Section 7.2, along
with a high level discussion of the memory transfer impact.  It is
worth noting that the Bercea paper is based upon the same Firedrake
technology of our article.

Referee: 3
==========

| Overall this is a well written and interesting paper, even to non-experts.
| However, as a non-expert, I have several concerns that I think should be
| addressed:

| 1) The focus is on minimizing flops, but on modern architectures, memory
| accesses are typically more expensive and flops are "almost free". Although
| clearly of theoretical interest, it would be nice to justify that this
| optimization also is important to reduce run time.

Figures 7, 8, 9, 10 show execution time speed-ups. Hence, we are already showing
that our work contributes to actually reducing run-times.

| 2) All experiments and results are run in serial on single core. In practice,
| most computers today are multicore (or manycore) so some discussion about
| extensions to multithreaded case would be appropriate. I recognize a parallel
| implementation is beyond the scope of the current paper.

We have added one paragraph in Section 9 and added a few sentences throughout
the text about this matter. Basically, in Firedrake parallelism is achieved at
the mesh level (by distributing mesh partitions to processes), while the
execution of a local assembly kernel is sequential. Clearly this might need to
be revisited in the case of higher order methods (as we would end up with big
working sets even for executing just a single element). This is discussed in
Section 9.

| 3) Results: The authors present several charts with speedups on different
| problems, which gives detailed information. However, it would be nice to add a
| figure with aggregate results, such as a scatter plot of all instances or a
| performance profile.

We agree with the referee about the usefulness of such a chart. We have added a
scatter plot (Fig. 11) as well as a section explaining what information we can
extrapolate from it.

| 4) The manuscript is already quite long so if possible, do not increase the
| total length (i.e. try find things to tighten or cut).

We tried our best, but we feel that we cannot really stay within 27
pages without losing essential information. We observe, however, that
this paper is within the typical range of page lengths in TOMS.
